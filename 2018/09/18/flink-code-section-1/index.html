<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="flink,流式计算," />










<meta name="description" content="Flink是大数据处理领域最近很火的一个开源的分布式、高性能的流式处理框架，其对数据的处理可以达到毫秒级别。本文以一个来自官网的WordCount例子为引，全面阐述flink的核心架构及执行流程，希望读者可以借此更加深入的理解Flink逻辑。">
<meta name="keywords" content="flink,流式计算">
<meta property="og:type" content="article">
<meta property="og:title" content="追源索骥：透过源码看懂Flink核心框架的执行流程">
<meta property="og:url" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/index.html">
<meta property="og:site_name" content="Xavier&#39;s Lab">
<meta property="og:description" content="Flink是大数据处理领域最近很火的一个开源的分布式、高性能的流式处理框架，其对数据的处理可以达到毫秒级别。本文以一个来自官网的WordCount例子为引，全面阐述flink的核心架构及执行流程，希望读者可以借此更加深入的理解Flink逻辑。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cae39t06eoo3ml1be8o0412c69.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cae7g15p6k94no1ves121c5pd9.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cenfj3p9fp110p0a8unn1mrh9.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1caf1oll019fp1odv1bh9idosr79.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1caf64b7c1gjnv2eebi1v9e1cvum.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1caf6ak4rkqsc1u1hci93fe0d13.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1caf71h79s0s3fodem1aeb1j3m1g.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafgsliu1n2n1uj21p971b0h6m71t.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafj7s6bittk5tt0bequlig2a.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafn516r1p68kt31g7r196rcsv2n.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafnu1pl1d8c15m219b8vkb2334.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cclle7ui2j41nf611gs1is18m19.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafpf21c1jh3s5ap1fisu4v23h.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafpko68b3r1lk0dpsnmbj3c3u.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cafqroarkjkuje1hfi18gor654b.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cag7idg4vfj1l871n0l1k0e1f7u4o.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cbkaa8r9182i18ct1kfu8g829m9.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cdc1tbgs136k1ppf17at14fumjf2d.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceop58ha180p1h3ren58jk15gb9.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceos05badva20hb5glen1voqm.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceot7q13apu1a04170af7j1jao34.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceot517e14g31u2u1mnt12o91dkb1g.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceot5kqbnik1f2i1dss1q5c1a1t.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceot64dppjtojkq3n1jl5j0h2a.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1ceot6kes56sidn1f2u1voo19kf2n.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cetavukjja42ce1261v5k57i9.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfmpba9v15anggtvsba2o1277m.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfksrl5cd4m1lbqqqgvc811349.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfkta54rkdd1od4aau1e3n7nhm.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfku114lf7hpqf3lmcl0116c13.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfku4g4g174d7gb5ecbfcib71g.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfkug5sm1v4l15pbgj4jntc7q1t.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfkvpmlh1gl31ef41cvh1c903a19.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfl05d2f1ub879c1lc5qsq14n9m.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cdbotdcmoe11q961st5lbn1j4n9.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cdbt8v5jl2ujn91uu1joh1p4gm.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cdbtcc5c1a6i1tuaadb1rd5136913.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cdbufp4a1opmsit5n61mial4520.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfl9453k1gld4acr1m13j3195sg.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfl9ju2617bh1s191mar1jsp12vot.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfla0n7u1lg21n3o36uu0c1o5h1a.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfla15os15i3qcsu6c4p4clk1n.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfla4tka101n18bf1mno4npu9s24.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cfla6eka1ph71mu1pll1q0mgqq2h.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cflaaim2ih2v54umsmq01lqc2u.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cflafs2o1trgicjmdbndn1bdq3b.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cflakoadvjm8pf6nt1k331qj33o.png">
<meta property="og:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cflambu91ufi5fl1cg9gimdff45.png">
<meta property="og:updated_time" content="2018-09-17T23:32:04.321Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="追源索骥：透过源码看懂Flink核心框架的执行流程">
<meta name="twitter:description" content="Flink是大数据处理领域最近很火的一个开源的分布式、高性能的流式处理框架，其对数据的处理可以达到毫秒级别。本文以一个来自官网的WordCount例子为引，全面阐述flink的核心架构及执行流程，希望读者可以借此更加深入的理解Flink逻辑。">
<meta name="twitter:image" content="http://blog.daas.ai/2018/09/18/flink-code-section-1/image_1cae39t06eoo3ml1be8o0412c69.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.daas.ai/2018/09/18/flink-code-section-1/"/>





  <title>追源索骥：透过源码看懂Flink核心框架的执行流程 | Xavier's Lab</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xavier's Lab</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">技术*产品*时光</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.daas.ai/2018/09/18/flink-code-section-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xavier ma">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xavier's Lab">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">追源索骥：透过源码看懂Flink核心框架的执行流程</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-18T06:33:31+08:00">
                2018-09-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/数据技术/" itemprop="url" rel="index">
                    <span itemprop="name">数据技术</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/18/flink-code-section-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/09/18/flink-code-section-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
              <div class="post-description">
                  Flink是大数据处理领域最近很火的一个开源的分布式、高性能的流式处理框架，其对数据的处理可以达到毫秒级别。本文以一个来自官网的WordCount例子为引，全面阐述flink的核心架构及执行流程，希望读者可以借此更加深入的理解Flink逻辑。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink是大数据处理领域一款开源的分布式、高性能的流式处理框架，它提供了一系列优雅的技术，实现对数据的毫秒级处理。本文以一个来自官网的WordCount例子为引，简单阐述flink的核心架构及执行流程，希望读者可以借此更加深入的理解Flink逻辑。</p>
<blockquote>
<p>本文跳过了一些基本概念，如果对相关概念感到迷惑，请参考官网文档。另外在本文写作过程中，Flink正式发布了其1.5 RELEASE版本，在其发布之后完成的内容将按照1.5的实现来组织。  </p>
</blockquote>
<h2 id="从-Hello-World-WordCount开始"><a href="#从-Hello-World-WordCount开始" class="headerlink" title="从 Hello,World WordCount开始"></a>从 Hello,World WordCount开始</h2><p>首先，我们把WordCount的例子再放一遍：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">public class SocketTextStreamWordCount &#123;</span><br><span class="line">    </span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        if (args.length != 2)&#123;</span><br><span class="line">            System.err.println(&quot;USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        String hostName = args[0];</span><br><span class="line">        Integer port = Integer.parseInt(args[1]);</span><br><span class="line">        // set up the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment</span><br><span class="line">                .getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        // get input data</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port);</span><br><span class="line">        </span><br><span class="line">        text.flatMap(new LineSplitter()).setParallelism(1)</span><br><span class="line">        // group by the tuple field &quot;0&quot; and sum up tuple field &quot;1&quot;</span><br><span class="line">                .keyBy(0)</span><br><span class="line">                .sum(1).setParallelism(1)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        // execute program</span><br><span class="line">        env.execute(&quot;Java WordCount from SocketTextStream Example&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">        /**</span><br><span class="line">         * Implements the string tokenizer that splits sentences into words as a user-defined</span><br><span class="line">         * FlatMapFunction. The function takes a line (String) and splits it into</span><br><span class="line">         * multiple pairs in the form of &quot;(word,1)&quot; (Tuple2&amp;lt;String, Integer&amp;gt;).</span><br><span class="line">         */</span><br><span class="line">        public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123;</span><br><span class="line">                // normalize and split the line</span><br><span class="line">                String[] tokens = value.toLowerCase().split(&quot;\\W+&quot;);</span><br><span class="line">                // emit the pairs</span><br><span class="line">                for (String token : tokens) &#123;</span><br><span class="line">                    if (token.length() &gt; 0) &#123;</span><br><span class="line">                        out.collect(new Tuple2&lt;String, Integer&gt;(token, 1));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>首先从命令行中获取socket对端的ip和端口，然后启动一个执行环境，从socket中读取数据，split成单个单词的流，并按单词进行总和的计数，最后打印出来。这个例子相信接触过大数据计算或者函数式编程的人都能看懂，就不过多解释了。</p>
<h3 id="flink执行环境"><a href="#flink执行环境" class="headerlink" title="flink执行环境"></a>flink执行环境</h3><p>程序的启动，从这句开始： <code>final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()</code> 。<br>这行代码会返回一个可用的执行环境。执行环境是整个flink程序执行的上下文，记录了相关配置（如并行度等），并提供了一系列方法，如读取输入流的方法，以及真正开始运行整个代码的execute方法等。对于分布式流处理程序来说，我们在代码中定义的flatMap,keyBy等等操作，事实上可以理解为一种声明，告诉整个程序我们采用了什么样的算子，而真正开启计算的代码不在此处。由于我们是在本地运行flink程序，因此这行代码会返回一个LocalStreamEnvironment，最后我们要调用它的execute方法来开启真正的任务。我们先接着往下看。</p>
<h3 id="算子（Operator）的注册（声明）"><a href="#算子（Operator）的注册（声明）" class="headerlink" title="算子（Operator）的注册（声明）"></a>算子（Operator）的注册（声明）</h3><p>我们以flatMap为例,<code>text.flatMap(new LineSplitter())</code> 这一句话跟踪进去是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; flatMap(FlatMapFunction&lt;T, R&gt; flatMapper) &#123;</span><br><span class="line"></span><br><span class="line">        TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper),</span><br><span class="line">                getType(), Utils.getCallLocationName(), true);</span><br><span class="line"></span><br><span class="line">        return transform(&quot;Flat Map&quot;, outType, new StreamFlatMap&lt;&gt;(clean(flatMapper)));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>里面完成了两件事，一是用反射拿到了flatMap算子的输出类型，二是生成了一个Operator。flink流式计算的核心概念，就是将数据从输入流一个个传递给Operator进行链式处理，最后交给输出流的过程。对数据的每一次处理在逻辑上成为一个operator，并且为了本地化处理的效率起见，operator之间也可以串成一个chain一起处理（可以参考责任链模式帮助理解）。下面这张图表明了flink是如何看待用户的处理流程的：抽象化为一系列operator，以source开始，以sink结尾，中间的operator做的操作叫做transform，并且可以把几个操作串在一起执行。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cae39t06eoo3ml1be8o0412c69.png" alt=""></p>
<p>我们也可以更改flink的设置，要求它不要对某个操作进行chain处理，或者从某个操作开启一个新chain等。<br>上面代码中的最后一行transform方法的作用是返回一个SingleOutputStreamOperator，它继承了Datastream类并且定义了一些辅助方法，方便对流的操作。在返回之前，transform方法还把它注册到了执行环境中（后面生成执行图的时候还会用到它）。其他的操作，包括keyBy，sum和print，都只是不同的算子，在这里出现都是一样的效果，即生成一个operator并注册给执行环境用于生成DAG。</p>
<h3 id="程序的执行"><a href="#程序的执行" class="headerlink" title="程序的执行"></a>程序的执行</h3><p>程序执行即 <code>env.execute(&quot;Java WordCount from SocketTextStream Example&quot;)</code> 这行代码。</p>
<h4 id="本地模式下的execute方法"><a href="#本地模式下的execute方法" class="headerlink" title="本地模式下的execute方法"></a>本地模式下的execute方法</h4><p>这行代码主要做了以下事情：</p>
<ul>
<li>生成StreamGraph。代表程序的拓扑结构，是从用户代码直接生成的图。</li>
<li>生成JobGraph。这个图是要交给flink去生成task的图。</li>
<li>生成一系列配置</li>
<li>将JobGraph和配置交给flink集群去运行。如果不是本地运行的话，还会把jar文件通过网络发给其他节点。</li>
<li>以本地模式运行的话，可以看到启动过程，如启动性能度量、web模块、JobManager、ResourceManager、taskManager等等</li>
<li>启动任务。值得一提的是在启动任务之前，先启动了一个用户类加载器，这个类加载器可以用来做一些在运行时动态加载类的工作。</li>
</ul>
<h4 id="远程模式（RemoteEnvironment）的execute方法"><a href="#远程模式（RemoteEnvironment）的execute方法" class="headerlink" title="远程模式（RemoteEnvironment）的execute方法"></a>远程模式（RemoteEnvironment）的execute方法</h4><p>远程模式的程序执行更加有趣一点。第一步仍然是获取StreamGraph，然后调用executeRemotely方法进行远程执行。<br>该方法首先创建一个用户代码加载器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ClassLoader usercodeClassLoader = JobWithJars.buildUserCodeClassLoader(jarFiles, globalClasspaths,   getClass().getClassLoader());</span><br></pre></td></tr></table></figure>
<p>然后创建一系列配置，交给Client对象。Client这个词有意思，看见它就知道这里绝对是跟远程集群打交道的客户端。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ClusterClient client;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            client = new StandaloneClusterClient(configuration);</span><br><span class="line">            client.setPrintStatusDuringExecution(getConfig().isSysoutLoggingEnabled());</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        try &#123;</span><br><span class="line">            return client.run(streamGraph, jarFiles, globalClasspaths, usercodeClassLoader).getJobExecutionResult();</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>client的run方法首先生成一个JobGraph，然后将其传递给JobClient。关于Client、JobClient、JobManager到底谁管谁，可以看这张图：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cae7g15p6k94no1ves121c5pd9.png" alt=""></p>
<p>确切的说，JobClient负责以异步的方式和JobManager通信（Actor是scala的异步模块），具体的通信任务由JobClientActor完成。相对应的，JobManager的通信任务也由一个Actor完成。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JobListeningContext jobListeningContext = submitJob(</span><br><span class="line">                actorSystem,config,highAvailabilityServices,jobGraph,timeout,sysoutLogUpdates,    classLoader);</span><br><span class="line"></span><br><span class="line">        return awaitJobResult(jobListeningContext);</span><br></pre></td></tr></table></figure>
<p>可以看到，该方法阻塞在awaitJobResult方法上，并最终返回了一个JobListeningContext，透过这个Context可以得到程序运行的状态和结果。</p>
<h4 id="程序启动过程"><a href="#程序启动过程" class="headerlink" title="程序启动过程"></a>程序启动过程</h4><p>上面提到，整个程序真正意义上开始执行，是这里：</p>
<ol>
<li><code>env.execute(&quot;Java WordCount from SocketTextStream Example&quot;);</code><br>远程模式和本地模式有一点不同，我们先按本地模式来调试。<br>我们跟进源码，（在本地调试模式下）会启动一个miniCluster，然后开始执行代码：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// LocalStreamEnvironment.java</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public JobExecutionResult execute(String jobName) throws Exception &#123;</span><br><span class="line">        </span><br><span class="line">        //生成各种图结构</span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            //启动集群，包括启动JobMaster，进行leader选举等等</span><br><span class="line">            miniCluster.start();</span><br><span class="line">            configuration.setInteger(RestOptions.PORT, miniCluster.getRestAddress().getPort());</span><br><span class="line">            </span><br><span class="line">            //提交任务到JobMaster</span><br><span class="line">            return miniCluster.executeJobBlocking(jobGraph);</span><br><span class="line">        &#125;</span><br><span class="line">        finally &#123;</span><br><span class="line">            transformations.clear();</span><br><span class="line">            miniCluster.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这个方法里有一部分逻辑是与生成图结构相关的，我们放在第二章里讲；现在我们先接着往里跟：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//MiniCluster.java</span><br><span class="line">public JobExecutionResult executeJobBlocking(JobGraph job) throws JobExecutionException, InterruptedException &#123;</span><br><span class="line">        checkNotNull(job, &quot;job is null&quot;);</span><br><span class="line">        </span><br><span class="line">        //在这里，最终把job提交给了jobMaster</span><br><span class="line">        final CompletableFuture&lt;JobSubmissionResult&gt; submissionFuture = submitJob(job);</span><br><span class="line"></span><br><span class="line">        final CompletableFuture&lt;JobResult&gt; jobResultFuture = submissionFuture.thenCompose(</span><br><span class="line">            (JobSubmissionResult ignored) -&gt; requestJobResult(job.getJobID()));</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>正如我在注释里写的，这一段代码核心逻辑就是调用那个 <code>submitJob</code> 方法。那么我们再接着看这个方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public CompletableFuture&lt;JobSubmissionResult&gt; submitJob(JobGraph jobGraph) &#123;</span><br><span class="line">        final DispatcherGateway dispatcherGateway;</span><br><span class="line">        try &#123;</span><br><span class="line">            dispatcherGateway = getDispatcherGateway();</span><br><span class="line">        &#125; catch (LeaderRetrievalException | InterruptedException e) &#123;</span><br><span class="line">            ExceptionUtils.checkInterrupted(e);</span><br><span class="line">            return FutureUtils.completedExceptionally(e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // we have to allow queued scheduling in Flip-6 mode because we need to request slots</span><br><span class="line">        // from the ResourceManager</span><br><span class="line">        jobGraph.setAllowQueuedScheduling(true);</span><br><span class="line"></span><br><span class="line">        final CompletableFuture&lt;Void&gt; jarUploadFuture = uploadAndSetJarFiles(dispatcherGateway, jobGraph);</span><br><span class="line"></span><br><span class="line">        final CompletableFuture&lt;Acknowledge&gt; acknowledgeCompletableFuture = jarUploadFuture.thenCompose(</span><br><span class="line">        </span><br><span class="line">        //在这里执行了真正的submit操作</span><br><span class="line">            (Void ack) -&gt; dispatcherGateway.submitJob(jobGraph, rpcTimeout));</span><br><span class="line"></span><br><span class="line">        return acknowledgeCompletableFuture.thenApply(</span><br><span class="line">            (Acknowledge ignored) -&gt; new JobSubmissionResult(jobGraph.getJobID()));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里的 <code>Dispatcher</code> 是一个接收job，然后指派JobMaster去启动任务的类,我们可以看看它的类结构，有两个实现。在本地环境下启动的是 <code>MiniDispatcher</code> ，在集群上提交任务时，集群上启动的是 <code>StandaloneDispatcher</code> 。 </p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cenfj3p9fp110p0a8unn1mrh9.png" alt=""><br>那么这个Dispatcher又做了什么呢？它启动了一个 <code>JobManagerRunner</code> （这里我要吐槽Flink的命名，这个东西应该叫做JobMasterRunner才对，flink里的JobMaster和JobManager不是一个东西），委托JobManagerRunner去启动该Job的 <code>JobMaster</code> 。我们看一下对应的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//jobManagerRunner.java</span><br><span class="line">    private void verifyJobSchedulingStatusAndStartJobManager(UUID leaderSessionId) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">        final CompletableFuture&lt;Acknowledge&gt; startFuture = jobMaster.start(new JobMasterId(leaderSessionId), rpcTimeout);</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>然后，JobMaster经过了一堆方法嵌套之后，执行到了这里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private void scheduleExecutionGraph() &#123;</span><br><span class="line">        checkState(jobStatusListener == null);</span><br><span class="line">        // register self as job status change listener</span><br><span class="line">        jobStatusListener = new JobManagerJobStatusListener();</span><br><span class="line">        executionGraph.registerJobStatusListener(jobStatusListener);</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            //这里调用了ExecutionGraph的启动方法</span><br><span class="line">            executionGraph.scheduleForExecution();</span><br><span class="line">        &#125;</span><br><span class="line">        catch (Throwable t) &#123;</span><br><span class="line">            executionGraph.failGlobal(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>我们知道，flink的框架里有三层图结构，其中ExecutionGraph就是真正被执行的那一层，所以到这里为止，一个任务从提交到真正执行的流程就走完了，我们再回顾一下（顺便提一下远程提交时的流程区别）：</p>
<ul>
<li>客户端代码的execute方法执行；</li>
<li>本地环境下，MiniCluster完成了大部分任务，直接把任务委派给了MiniDispatcher；</li>
<li>远程环境下，启动了一个 <code>RestClusterClient</code> ，这个类会以HTTP Rest的方式把用户代码提交到集群上；</li>
<li>远程环境下，请求发到集群上之后，必然有个handler去处理，在这里是 <code>JobSubmitHandler</code> 。这个类接手了请求后，委派StandaloneDispatcher启动job，到这里之后，本地提交和远程提交的逻辑往后又统一了；</li>
<li>Dispatcher接手job之后，会实例化一个 <code>JobManagerRunner</code> ，然后用这个runner启动job；</li>
<li>JobManagerRunner接下来把job交给了 <code>JobMaster</code> 去处理；</li>
<li>JobMaster使用 <code>ExecutionGraph</code> 的方法启动了整个执行图；整个任务就启动起来了。<br>至此，第一部分就讲完了。</li>
</ul>
<h2 id="理解flink的图结构"><a href="#理解flink的图结构" class="headerlink" title="理解flink的图结构"></a>理解flink的图结构</h2><p>第一部分讲到，我们的主函数最后一项任务就是生成StreamGraph，然后生成JobGraph，然后以此开始调度任务运行，所以接下来我们从这里入手，继续探索flink。</p>
<h3 id="flink的三层图结构"><a href="#flink的三层图结构" class="headerlink" title="flink的三层图结构"></a>flink的三层图结构</h3><p>事实上，flink总共提供了三种图的抽象，我们前面已经提到了StreamGraph和JobGraph，还有一种是ExecutionGraph，是用于调度的基本数据结构。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1caf1oll019fp1odv1bh9idosr79.png" alt=""></p>
<p>上面这张图清晰的给出了flink各个图的工作原理和转换过程。其中最后一个物理执行图并非flink的数据结构，而是程序开始执行后，各个task分布在不同的节点上，所形成的物理上的关系表示。</p>
<ul>
<li>从JobGraph的图里可以看到，数据从上一个operator流到下一个operator的过程中，上游作为生产者提供了IntermediateDataSet，而下游作为消费者需要JobEdge。事实上，JobEdge是一个通信管道，连接了上游生产的dataset和下游的JobVertex节点。</li>
<li><p>在JobGraph转换到ExecutionGraph的过程中，主要发生了以下转变：</p>
<ul>
<li>加入了并行度的概念，成为真正可调度的图结构</li>
<li>生成了与JobVertex对应的ExecutionJobVertex，ExecutionVertex，与IntermediateDataSet对应的IntermediateResult和IntermediateResultPartition等，并行将通过这些类实现</li>
</ul>
</li>
<li><p>ExecutionGraph已经可以用于调度任务。我们可以看到，flink根据该图生成了一一对应的Task，每个task对应一个ExecutionGraph的一个Execution。Task用InputGate、InputChannel和ResultPartition对应了上面图中的IntermediateResult和ExecutionEdge。<br>那么，flink抽象出这三层图结构，四层执行逻辑的意义是什么呢？<br>StreamGraph是对用户逻辑的映射。JobGraph在此基础上进行了一些优化，比如把一部分操作串成chain以提高效率。ExecutionGraph是为了调度存在的，加入了并行处理的概念。而在此基础上真正执行的是Task及其相关结构。</p>
</li>
</ul>
<h3 id="StreamGraph的生成"><a href="#StreamGraph的生成" class="headerlink" title="StreamGraph的生成"></a>StreamGraph的生成</h3><p>在第一节的算子注册部分，我们可以看到，flink把每一个算子transform成一个对流的转换（比如上文中返回的SingleOutputStreamOperator是一个DataStream的子类），并且注册到执行环境中，用于生成StreamGraph。实际生成StreamGraph的入口是 <code>StreamGraphGenerator.generate(env, transformations)</code> 其中的transformations是一个list，里面记录的就是我们在transform方法中放进来的算子。</p>
<h4 id="StreamTransformation类代表了流的转换"><a href="#StreamTransformation类代表了流的转换" class="headerlink" title="StreamTransformation类代表了流的转换"></a>StreamTransformation类代表了流的转换</h4><p>StreamTransformation代表了从一个或多个DataStream生成新DataStream的操作。顺便，DataStream类在内部组合了一个StreamTransformation类，实际的转换操作均通过该类完成。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1caf64b7c1gjnv2eebi1v9e1cvum.png" alt=""></p>
<p>我们可以看到，从source到各种map,union再到sink操作全部被映射成了StreamTransformation。<br>其映射过程如下所示：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1caf6ak4rkqsc1u1hci93fe0d13.png" alt=""><br>以MapFunction为例：</p>
<ul>
<li>首先，用户代码里定义的UDF会被当作其基类对待，然后交给StreamMap这个operator做进一步包装。事实上，每一个Transformation都对应了一个StreamOperator。</li>
<li>由于map这个操作只接受一个输入，所以再被进一步包装为OneInputTransformation。</li>
<li><p>最后，将该transformation注册到执行环境中，当执行上文提到的generate方法时，生成StreamGraph图结构。</p>
</li>
<li><p>另外，并不是每一个 StreamTransformation 都会转换成runtime层中的物理操作。有一些只是逻辑概念，比如union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p>
</li>
</ul>
<p><img src="/2018/09/18/flink-code-section-1/image_1caf71h79s0s3fodem1aeb1j3m1g.png" alt=""></p>
<h4 id="StreamGraph生成函数分析"><a href="#StreamGraph生成函数分析" class="headerlink" title="StreamGraph生成函数分析"></a>StreamGraph生成函数分析</h4><p>我们从StreamGraphGenerator.generate()方法往下看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">public static StreamGraph generate(StreamExecutionEnvironment env, List&lt;StreamTransformation&lt;?&gt;&gt; transformations) &#123;</span><br><span class="line">        return new StreamGraphGenerator(env).generateInternal(transformations);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //注意，StreamGraph的生成是从sink开始的</span><br><span class="line">    private StreamGraph generateInternal(List&lt;StreamTransformation&lt;?&gt;&gt; transformations) &#123;</span><br><span class="line">        for (StreamTransformation&lt;?&gt; transformation: transformations) &#123;</span><br><span class="line">            transform(transformation);</span><br><span class="line">        &#125;</span><br><span class="line">        return streamGraph;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //这个方法的核心逻辑就是判断传入的steamOperator是哪种类型，并执行相应的操作，详情见下面那一大堆if-else</span><br><span class="line">    private Collection&lt;Integer&gt; transform(StreamTransformation&lt;?&gt; transform) &#123;</span><br><span class="line"></span><br><span class="line">        if (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">            return alreadyTransformed.get(transform);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        LOG.debug(&quot;Transforming &quot; + transform);</span><br><span class="line"></span><br><span class="line">        if (transform.getMaxParallelism() &lt;= 0) &#123;</span><br><span class="line"></span><br><span class="line">            // if the max parallelism hasn&apos;t been set, then first use the job wide max parallelism</span><br><span class="line">            // from theExecutionConfig.</span><br><span class="line">            int globalMaxParallelismFromConfig = env.getConfig().getMaxParallelism();</span><br><span class="line">            if (globalMaxParallelismFromConfig &gt; 0) &#123;</span><br><span class="line">                transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // call at least once to trigger exceptions about MissingTypeInfo</span><br><span class="line">        transform.getOutputType();</span><br><span class="line"></span><br><span class="line">        Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">        //这里对操作符的类型进行判断，并以此调用相应的处理逻辑.简而言之，处理的核心无非是递归的将该节点和节点的上游节点加入图</span><br><span class="line">        if (transform instanceof OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">            transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">            transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else if (transform instanceof SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">            transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            throw new IllegalStateException(&quot;Unknown transformation: &quot; + transform);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        //注意这里和函数开始时的方法相对应，在有向图中要注意避免循环的产生</span><br><span class="line">        // need this check because the iterate transformation adds itself before</span><br><span class="line">        // transforming the feedback edges</span><br><span class="line">        if (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">            alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (transform.getBufferTimeout() &gt; 0) &#123;</span><br><span class="line">            streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">        &#125;</span><br><span class="line">        if (transform.getUid() != null) &#123;</span><br><span class="line">            streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">        &#125;</span><br><span class="line">        if (transform.getUserProvidedNodeHash() != null) &#123;</span><br><span class="line">            streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (transform.getMinResources() != null &amp;&amp; transform.getPreferredResources() != null) &#123;</span><br><span class="line">            streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return transformedIds;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>因为map，filter等常用操作都是OneInputStreamOperator,我们就来看看 <code>transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform)</code> 方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">private &lt;IN, OUT&gt; Collection&lt;Integer&gt; transformOneInputTransform(OneInputTransformation&lt;IN, OUT&gt; transform) &#123;</span><br><span class="line"></span><br><span class="line">        Collection&lt;Integer&gt; inputIds = transform(transform.getInput());</span><br><span class="line"></span><br><span class="line">        // 在递归处理节点过程中，某个节点可能已经被其他子节点先处理过了，需要跳过</span><br><span class="line">        if (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">            return alreadyTransformed.get(transform);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        //这里是获取slotSharingGroup。这个group用来定义当前我们在处理的这个操作符可以跟什么操作符chain到一个slot里进行操作</span><br><span class="line">        //因为有时候我们可能不满意flink替我们做的chain聚合</span><br><span class="line">        //一个slot就是一个执行task的基本容器</span><br><span class="line">        String slotSharingGroup = determineSlotSharingGroup(transform.getSlotSharingGroup(), inputIds);</span><br><span class="line"></span><br><span class="line">        //把该operator加入图</span><br><span class="line">        streamGraph.addOperator(transform.getId(),</span><br><span class="line">                slotSharingGroup,</span><br><span class="line">                transform.getOperator(),</span><br><span class="line">                transform.getInputType(),</span><br><span class="line">                transform.getOutputType(),</span><br><span class="line">                transform.getName());</span><br><span class="line">        </span><br><span class="line">        //对于keyedStream，我们还要记录它的keySelector方法</span><br><span class="line">        //flink并不真正为每个keyedStream保存一个key，而是每次需要用到key的时候都使用keySelector方法进行计算</span><br><span class="line">        //因此，我们自定义的keySelector方法需要保证幂等性</span><br><span class="line">        //到后面介绍keyGroup的时候我们还会再次提到这一点</span><br><span class="line">        if (transform.getStateKeySelector() != null) &#123;</span><br><span class="line">            TypeSerializer&lt;?&gt; keySerializer = transform.getStateKeyType().createSerializer(env.getConfig());</span><br><span class="line">            streamGraph.setOneInputStateKey(transform.getId(), transform.getStateKeySelector(), keySerializer);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        streamGraph.setParallelism(transform.getId(), transform.getParallelism());</span><br><span class="line">        streamGraph.setMaxParallelism(transform.getId(), transform.getMaxParallelism());</span><br><span class="line">        </span><br><span class="line">        //为当前节点和它的依赖节点建立边</span><br><span class="line">        //这里可以看到之前提到的select union partition等逻辑节点被合并入edge的过程</span><br><span class="line">        for (Integer inputId: inputIds) &#123;</span><br><span class="line">            streamGraph.addEdge(inputId, transform.getId(), 0);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return Collections.singleton(transform.getId());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void addEdge(Integer upStreamVertexID, Integer downStreamVertexID, int typeNumber) &#123;</span><br><span class="line">        addEdgeInternal(upStreamVertexID,</span><br><span class="line">                downStreamVertexID,</span><br><span class="line">                typeNumber,</span><br><span class="line">                null,</span><br><span class="line">                new ArrayList&lt;String&gt;(),</span><br><span class="line">                null);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    //addEdge的实现，会合并一些逻辑节点</span><br><span class="line">    private void addEdgeInternal(Integer upStreamVertexID,</span><br><span class="line">            Integer downStreamVertexID,</span><br><span class="line">            int typeNumber,</span><br><span class="line">            StreamPartitioner&lt;?&gt; partitioner,</span><br><span class="line">            List&lt;String&gt; outputNames,</span><br><span class="line">            OutputTag outputTag) &#123;</span><br><span class="line">        //如果输入边是侧输出节点，则把side的输入边作为本节点的输入边，并递归调用</span><br><span class="line">        if (virtualSideOutputNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">            int virtualId = upStreamVertexID;</span><br><span class="line">            upStreamVertexID = virtualSideOutputNodes.get(virtualId).f0;</span><br><span class="line">            if (outputTag == null) &#123;</span><br><span class="line">                outputTag = virtualSideOutputNodes.get(virtualId).f1;</span><br><span class="line">            &#125;</span><br><span class="line">            addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, null, outputTag);</span><br><span class="line">            //如果输入边是select，则把select的输入边作为本节点的输入边</span><br><span class="line">        &#125; else if (virtualSelectNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">            int virtualId = upStreamVertexID;</span><br><span class="line">            upStreamVertexID = virtualSelectNodes.get(virtualId).f0;</span><br><span class="line">            if (outputNames.isEmpty()) &#123;</span><br><span class="line">                // selections that happen downstream override earlier selections</span><br><span class="line">                outputNames = virtualSelectNodes.get(virtualId).f1;</span><br><span class="line">            &#125;</span><br><span class="line">            addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames, outputTag);</span><br><span class="line">            //如果是partition节点</span><br><span class="line">        &#125; else if (virtualPartitionNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">            int virtualId = upStreamVertexID;</span><br><span class="line">            upStreamVertexID = virtualPartitionNodes.get(virtualId).f0;</span><br><span class="line">            if (partitioner == null) &#123;</span><br><span class="line">                partitioner = virtualPartitionNodes.get(virtualId).f1;</span><br><span class="line">            &#125;</span><br><span class="line">            addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames, outputTag);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        //正常的edge处理逻辑</span><br><span class="line">            StreamNode upstreamNode = getStreamNode(upStreamVertexID);</span><br><span class="line">            StreamNode downstreamNode = getStreamNode(downStreamVertexID);</span><br><span class="line"></span><br><span class="line">            // If no partitioner was specified and the parallelism of upstream and downstream</span><br><span class="line">            // operator matches use forward partitioning, use rebalance otherwise.</span><br><span class="line">            if (partitioner == null &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">                partitioner = new ForwardPartitioner&lt;Object&gt;();</span><br><span class="line">            &#125; else if (partitioner == null) &#123;</span><br><span class="line">                partitioner = new RebalancePartitioner&lt;Object&gt;();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (partitioner instanceof ForwardPartitioner) &#123;</span><br><span class="line">                if (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line">                    throw new UnsupportedOperationException(&quot;Forward partitioning does not allow &quot; +</span><br><span class="line">                            &quot;change of parallelism. Upstream operation: &quot; + upstreamNode + &quot; parallelism: &quot; + upstreamNode.getParallelism() +</span><br><span class="line">                            &quot;, downstream operation: &quot; + downstreamNode + &quot; parallelism: &quot; + downstreamNode.getParallelism() +</span><br><span class="line">                            &quot; You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            StreamEdge edge = new StreamEdge(upstreamNode, downstreamNode, typeNumber, outputNames, partitioner, outputTag);</span><br><span class="line"></span><br><span class="line">            getStreamNode(edge.getSourceId()).addOutEdge(edge);</span><br><span class="line">            getStreamNode(edge.getTargetId()).addInEdge(edge);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="WordCount函数的StreamGraph"><a href="#WordCount函数的StreamGraph" class="headerlink" title="WordCount函数的StreamGraph"></a>WordCount函数的StreamGraph</h4><p>flink提供了一个StreamGraph可视化显示工具， <a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">在这里</a><br>我们可以把我们的程序的执行计划打印出来 <code>System.out.println(env.getExecutionPlan());</code> 复制到这个网站上，点击生成，如图所示： </p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafgsliu1n2n1uj21p971b0h6m71t.png" alt=""></p>
<p>可以看到，我们源程序被转化成了4个operator。<br>另外，在operator之间的连线上也显示出了flink添加的一些逻辑流程。由于我设定了每个操作符的并行度都是1，所以在每个操作符之间都是直接FORWARD，不存在shuffle的过程。</p>
<h3 id="JobGraph的生成"><a href="#JobGraph的生成" class="headerlink" title="JobGraph的生成"></a>JobGraph的生成</h3><p>flink会根据上一步生成的StreamGraph生成JobGraph，然后将JobGraph发送到server端进行ExecutionGraph的解析。</p>
<h4 id="JobGraph生成源码"><a href="#JobGraph生成源码" class="headerlink" title="JobGraph生成源码"></a>JobGraph生成源码</h4><p>与StreamGraph类似，JobGraph的入口方法是 <code>StreamingJobGraphGenerator.createJobGraph()</code> 。我们直接来看源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">private JobGraph createJobGraph() &#123;</span><br><span class="line"></span><br><span class="line">        // 设置启动模式为所有节点均在一开始就启动</span><br><span class="line">        jobGraph.setScheduleMode(ScheduleMode.EAGER);</span><br><span class="line"></span><br><span class="line">        // 为每个节点生成hash id</span><br><span class="line">        Map&lt;Integer, byte[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph);</span><br><span class="line"></span><br><span class="line">        // 为了保持兼容性创建的hash</span><br><span class="line">        List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes = new ArrayList&lt;&gt;(legacyStreamGraphHashers.size());</span><br><span class="line">        for (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123;</span><br><span class="line">            legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes = new HashMap&lt;&gt;();</span><br><span class="line">        //生成jobvertex，串成chain等</span><br><span class="line">        //这里的逻辑大致可以理解为，挨个遍历节点，如果该节点是一个chain的头节点，就生成一个JobVertex，如果不是头节点，就要把自身配置并入头节点，然后把头节点和自己的出边相连；对于不能chain的节点，当作只有头节点处理即可</span><br><span class="line">        setChaining(hashes, legacyHashes, chainedOperatorHashes);</span><br><span class="line">        //设置输入边edge</span><br><span class="line">        setPhysicalEdges();</span><br><span class="line">        //设置slot共享group</span><br><span class="line">        setSlotSharing();</span><br><span class="line">        //配置检查点</span><br><span class="line">        configureCheckpointing();</span><br><span class="line"></span><br><span class="line">        // 如果有之前的缓存文件的配置的话，重新读入</span><br><span class="line">        for (Tuple2&lt;String, DistributedCache.DistributedCacheEntry&gt; e : streamGraph.getEnvironment().getCachedFiles()) &#123;</span><br><span class="line">            DistributedCache.writeFileInfoToConfig(e.f0, e.f1, jobGraph.getJobConfiguration());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 传递执行环境配置</span><br><span class="line">        try &#123;</span><br><span class="line">            jobGraph.setExecutionConfig(streamGraph.getExecutionConfig());</span><br><span class="line">        &#125;</span><br><span class="line">        catch (IOException e) &#123;</span><br><span class="line">            throw new IllegalConfigurationException(&quot;Could not serialize the ExecutionConfig.&quot; +</span><br><span class="line">                    &quot;This indicates that non-serializable types (like custom serializers) were registered&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return jobGraph;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="operator-chain的逻辑"><a href="#operator-chain的逻辑" class="headerlink" title="operator chain的逻辑"></a>operator chain的逻辑</h4><blockquote>
<p>为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。  </p>
</blockquote>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafj7s6bittk5tt0bequlig2a.png" alt=""></p>
<p>上图中将KeyAggregation和Sink两个operator进行了合并，因为这两个合并后并不会改变整体的拓扑结构。但是，并不是任意两个 operator 就能 chain 一起的,其条件还是很苛刻的：</p>
<ul>
<li>上下游的并行度一致</li>
<li>下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</li>
<li>上下游节点都在同一个 slot group 中（下面会解释 slot group）</li>
<li>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</li>
<li>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</li>
<li>两个节点间数据分区方式是 forward（参考理解数据流的分区）</li>
<li>用户没有禁用 chain<br>flink的chain逻辑是一种很常见的设计，比如spring的interceptor也是类似的实现方式。通过把操作符串成一个大操作符，flink避免了把数据序列化后通过网络发送给其他节点的开销，能够大大增强效率。</li>
</ul>
<h4 id="2-3-3-JobGraph的提交"><a href="#2-3-3-JobGraph的提交" class="headerlink" title="2.3.3 JobGraph的提交"></a>2.3.3 JobGraph的提交</h4><p>前面已经提到，JobGraph的提交依赖于JobClient和JobManager之间的异步通信，如图所示：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafn516r1p68kt31g7r196rcsv2n.png" alt=""></p>
<p>在submitJobAndWait方法中，其首先会创建一个JobClientActor的ActorRef,然后向其发起一个SubmitJobAndWait消息，该消息将JobGraph的实例提交给JobClientActor。发起模式是ask，它表示需要一个应答消息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;Object&gt; future = Patterns.ask(jobClientActor, new JobClientMessages.SubmitJobAndWait(jobGraph), new Timeout(AkkaUtils.INF_TIMEOUT()));</span><br><span class="line">answer = Await.result(future, AkkaUtils.INF_TIMEOUT());</span><br></pre></td></tr></table></figure>
<p>该SubmitJobAndWait消息被JobClientActor接收后，最终通过调用tryToSubmitJob方法触发真正的提交动作。当JobManager的actor接收到来自client端的请求后，会执行一个submitJob方法，主要做以下事情：</p>
<ul>
<li>向BlobLibraryCacheManager注册该Job；</li>
<li>构建ExecutionGraph对象；</li>
<li>对JobGraph中的每个顶点进行初始化；</li>
<li>将DAG拓扑中从source开始排序，排序后的顶点集合附加到Exec&gt; - utionGraph对象；</li>
<li>获取检查点相关的配置，并将其设置到ExecutionGraph对象；</li>
<li>向ExecutionGraph注册相关的listener；</li>
<li>执行恢复操作或者将JobGraph信息写入SubmittedJobGraphStore以在后续用于恢复目的；</li>
<li>响应给客户端JobSubmitSuccess消息；</li>
<li>对ExecutionGraph对象进行调度执行；<br>最后，JobManger会返回消息给JobClient，通知该任务是否提交成功。</li>
</ul>
<h3 id="ExecutionGraph的生成"><a href="#ExecutionGraph的生成" class="headerlink" title="ExecutionGraph的生成"></a>ExecutionGraph的生成</h3><p>与StreamGraph和JobGraph不同，ExecutionGraph并不是在我们的客户端程序生成，而是在服务端（JobManager处）生成的，顺便flink只维护一个JobManager。其入口代码是 <code>ExecutionGraphBuilder.buildGraph（...）</code><br>该方法长200多行，其中一大半是checkpoiont的相关逻辑，我们暂且略过，直接看核心方法 <code>executionGraph.attachJobGraph(sortedTopology)</code><br>因为ExecutionGraph事实上只是改动了JobGraph的每个节点，而没有对整个拓扑结构进行变动，所以代码里只是挨个遍历jobVertex并进行处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">for (JobVertex jobVertex : topologiallySorted) &#123;</span><br><span class="line"></span><br><span class="line">            if (jobVertex.isInputVertex() &amp;&amp; !jobVertex.isStoppable()) &#123;</span><br><span class="line">                this.isStoppable = false;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            //在这里生成ExecutionGraph的每个节点</span><br><span class="line">            //首先是进行了一堆赋值，将任务信息交给要生成的图节点，以及设定并行度等等</span><br><span class="line">            //然后是创建本节点的IntermediateResult，根据本节点的下游节点的个数确定创建几份</span><br><span class="line">            //最后是根据设定好的并行度创建用于执行task的ExecutionVertex</span><br><span class="line">            //如果job有设定inputsplit的话，这里还要指定inputsplits</span><br><span class="line">            ExecutionJobVertex ejv = new ExecutionJobVertex(</span><br><span class="line">                this,</span><br><span class="line">                jobVertex,</span><br><span class="line">                1,</span><br><span class="line">                rpcCallTimeout,</span><br><span class="line">                globalModVersion,</span><br><span class="line">                createTimestamp);</span><br><span class="line">            </span><br><span class="line">            //这里要处理所有的JobEdge</span><br><span class="line">            //对每个edge，获取对应的intermediateResult，并记录到本节点的输入上</span><br><span class="line">            //最后，把每个ExecutorVertex和对应的IntermediateResult关联起来</span><br><span class="line">            ejv.connectToPredecessors(this.intermediateResults);</span><br><span class="line"></span><br><span class="line">            ExecutionJobVertex previousTask = this.tasks.putIfAbsent(jobVertex.getID(), ejv);</span><br><span class="line">            if (previousTask != null) &#123;</span><br><span class="line">                throw new JobException(String.format(&quot;Encountered two job vertices with ID %s : previous=[%s] / new=[%s]&quot;,</span><br><span class="line">                        jobVertex.getID(), ejv, previousTask));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            for (IntermediateResult res : ejv.getProducedDataSets()) &#123;</span><br><span class="line">                IntermediateResult previousDataSet = this.intermediateResults.putIfAbsent(res.getId(), res);</span><br><span class="line">                if (previousDataSet != null) &#123;</span><br><span class="line">                    throw new JobException(String.format(&quot;Encountered two intermediate data set with ID %s : previous=[%s] / new=[%s]&quot;,</span><br><span class="line">                            res.getId(), res, previousDataSet));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            this.verticesInCreationOrder.add(ejv);</span><br><span class="line">            this.numVerticesTotal += ejv.getParallelism();</span><br><span class="line">            newExecJobVertices.add(ejv);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>至此，ExecutorGraph就创建完成了。</p>
<h2 id="任务的调度与执行"><a href="#任务的调度与执行" class="headerlink" title="任务的调度与执行"></a>任务的调度与执行</h2><p>关于flink的任务执行架构，官网的这两张图就是最好的说明：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafnu1pl1d8c15m219b8vkb2334.png" alt=""></p>
<p>Flink 集群启动后，首先会启动一个 JobManger 和多个的 TaskManager。用户的代码会由JobClient 提交给 JobManager，JobManager 再把来自不同用户的任务发给 不同的TaskManager 去执行，每个TaskManager管理着多个task，task是执行计算的最小结构， TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述除了task外的三者均为独立的 JVM 进程。<br>要注意的是，TaskManager和job并非一一对应的关系。flink调度的最小单元是task而非TaskManager，也就是说，来自不同job的不同task可能运行于同一个TaskManager的不同线程上。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cclle7ui2j41nf611gs1is18m19.png" alt=""></p>
<p>一个flink任务所有可能的状态如上图所示。图上画的很明白，就不再赘述了。</p>
<h3 id="计算资源的调度"><a href="#计算资源的调度" class="headerlink" title="计算资源的调度"></a>计算资源的调度</h3><p>Task slot是一个TaskManager内资源分配的最小载体，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。<br>通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。<br>而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。<br>每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafpf21c1jh3s5ap1fisu4v23h.png" alt=""></p>
<p>为了达到共用slot的目的，除了可以以chain的方式pipeline算子，我们还可以允许SlotSharingGroup，如下图所示：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafpko68b3r1lk0dpsnmbj3c3u.png" alt=""></p>
<p>我们可以把不能被chain成一条的两个操作如flatmap和key&amp;sink放在一个TaskSlot里执行，这样做可以获得以下好处：</p>
<ul>
<li>共用slot使得我们不再需要计算每个任务需要的总task数目，直接取最高算子的并行度即可</li>
<li>对计算资源的利用率更高。例如，通常的轻量级操作map和重量级操作Aggregate不再分别需要一个线程，而是可以在同一个线程内执行，而且对于slot有限的场景，我们可以增大每个task的并行度了。</li>
<li>接下来我们还是用官网的图来说明flink是如何重用slot的：</li>
</ul>
<p><img src="/2018/09/18/flink-code-section-1/image_1cafqroarkjkuje1hfi18gor654b.png" alt=""></p>
<pre><code>1. TaskManager1分配一个SharedSlot0
2. 把source task放入一个SimpleSlot0，再把该slot放入SharedSlot0
3. 把flatmap task放入一个SimpleSlot1，再把该slot放入SharedSlot0
4. 因为我们的flatmap task并行度是2，因此不能再放入SharedSlot0，所以向TaskMange21申请了一个新的SharedSlot0
5. 把第二个flatmap task放进一个新的SimpleSlot，并放进TaskManager2的SharedSlot0
6. 开始处理key&amp;sink task，因为其并行度也是2，所以先把第一个task放进TaskManager1的SharedSlot
7. 把第二个key&amp;sink放进TaskManager2的SharedSlot
</code></pre><h3 id="JobManager执行job"><a href="#JobManager执行job" class="headerlink" title="JobManager执行job"></a>JobManager执行job</h3><p>JobManager负责接收 flink 的作业，调度 task，收集 job 的状态、管理 TaskManagers。被实现为一个 akka actor。</p>
<h4 id="JobManager的组件"><a href="#JobManager的组件" class="headerlink" title="JobManager的组件"></a>JobManager的组件</h4><ul>
<li>BlobServer 是一个用来管理二进制大文件的服务，比如保存用户上传的jar文件，该服务会将其写到磁盘上。还有一些相关的类，如BlobCache，用于TaskManager向JobManager下载用户的jar文件</li>
<li>InstanceManager 用来管理当前存活的TaskManager的组件，记录了TaskManager的心跳信息等</li>
<li>CompletedCheckpointStore 用于保存已完成的checkpoint相关信息，持久化到内存中或者zookeeper上</li>
<li>MemoryArchivist 保存了已经提交到flink的作业的相关信息，如JobGraph等</li>
</ul>
<h4 id="JobManager的启动过程"><a href="#JobManager的启动过程" class="headerlink" title="JobManager的启动过程"></a>JobManager的启动过程</h4><p>先列出JobManager启动的核心代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def runJobManager(</span><br><span class="line">      configuration: Configuration,</span><br><span class="line">      executionMode: JobManagerMode,</span><br><span class="line">      listeningAddress: String,</span><br><span class="line">      listeningPort: Int)</span><br><span class="line">    : Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val numberProcessors = Hardware.getNumberCPUCores()</span><br><span class="line"></span><br><span class="line">    val futureExecutor = Executors.newScheduledThreadPool(</span><br><span class="line">      numberProcessors,</span><br><span class="line">      new ExecutorThreadFactory(&quot;jobmanager-future&quot;))</span><br><span class="line"></span><br><span class="line">    val ioExecutor = Executors.newFixedThreadPool(</span><br><span class="line">      numberProcessors,</span><br><span class="line">      new ExecutorThreadFactory(&quot;jobmanager-io&quot;))</span><br><span class="line"></span><br><span class="line">    val timeout = AkkaUtils.getTimeout(configuration)</span><br><span class="line"></span><br><span class="line">    // we have to first start the JobManager ActorSystem because this determines the port if 0</span><br><span class="line">    // was chosen before. The method startActorSystem will update the configuration correspondingly.</span><br><span class="line">    val jobManagerSystem = startActorSystem(</span><br><span class="line">      configuration,</span><br><span class="line">      listeningAddress,</span><br><span class="line">      listeningPort)</span><br><span class="line"></span><br><span class="line">    val highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(</span><br><span class="line">      configuration,</span><br><span class="line">      ioExecutor,</span><br><span class="line">      AddressResolution.NO_ADDRESS_RESOLUTION)</span><br><span class="line"></span><br><span class="line">    val metricRegistry = new MetricRegistryImpl(</span><br><span class="line">      MetricRegistryConfiguration.fromConfiguration(configuration))</span><br><span class="line"></span><br><span class="line">    metricRegistry.startQueryService(jobManagerSystem, null)</span><br><span class="line"></span><br><span class="line">    val (_, _, webMonitorOption, _) = try &#123;</span><br><span class="line">      startJobManagerActors(</span><br><span class="line">        jobManagerSystem,</span><br><span class="line">        configuration,</span><br><span class="line">        executionMode,</span><br><span class="line">        listeningAddress,</span><br><span class="line">        futureExecutor,</span><br><span class="line">        ioExecutor,</span><br><span class="line">        highAvailabilityServices,</span><br><span class="line">        metricRegistry,</span><br><span class="line">        classOf[JobManager],</span><br><span class="line">        classOf[MemoryArchivist],</span><br><span class="line">        Option(classOf[StandaloneResourceManager])</span><br><span class="line">      )</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case t: Throwable =&gt;</span><br><span class="line">        futureExecutor.shutdownNow()</span><br><span class="line">        ioExecutor.shutdownNow()</span><br><span class="line"></span><br><span class="line">        throw t</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // block until everything is shut down</span><br><span class="line">    jobManagerSystem.awaitTermination()</span><br><span class="line">    </span><br><span class="line">    .......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>配置Akka并生成ActorSystem，启动JobManager</li>
<li>启动HA和metric相关服务</li>
<li>在 <code>startJobManagerActors()</code> 方法中启动JobManagerActors，以及webserver，TaskManagerActor，ResourceManager等等</li>
<li>阻塞等待终止</li>
<li>集群通过LeaderService等选出JobManager的leader</li>
</ul>
<h4 id="JobManager启动Task"><a href="#JobManager启动Task" class="headerlink" title="JobManager启动Task"></a>JobManager启动Task</h4><p>JobManager 是一个Actor，通过各种消息来完成核心逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">override def handleMessage: Receive = &#123;</span><br><span class="line">  case GrantLeadership(newLeaderSessionID) =&gt;</span><br><span class="line">    log.info(s&quot;JobManager $getAddress was granted leadership with leader session ID &quot; +</span><br><span class="line">      s&quot;$newLeaderSessionID.&quot;)</span><br><span class="line">    leaderSessionID = newLeaderSessionID</span><br><span class="line">    </span><br><span class="line">    .......</span><br></pre></td></tr></table></figure>
<p>有几个比较重要的消息：</p>
<ul>
<li>GrantLeadership 获得leader授权，将自身被分发到的 session id 写到 zookeeper，并恢复所有的 jobs</li>
<li>RevokeLeadership 剥夺leader授权，打断清空所有的 job 信息，但是保留作业缓存，注销所有的 TaskManagers</li>
<li>RegisterTaskManagers 注册 TaskManager，如果之前已经注册过，则只给对应的 Instance 发送消息，否则启动注册逻辑：在 InstanceManager 中注册该 Instance 的信息，并停止 Instance BlobLibraryCacheManager 的端口【供下载 lib 包用】，同时使用 watch 监听 task manager 的存活</li>
<li>SubmitJob 提交 jobGraph</li>
<li>最后一项SubmintJob就是我们要关注的，从客户端收到JobGraph，转换为ExecutionGraph并执行的过程。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">private def submitJob(jobGraph: JobGraph, jobInfo: JobInfo, isRecovery: Boolean = false): Unit = &#123;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    executionGraph = ExecutionGraphBuilder.buildGraph(</span><br><span class="line">          executionGraph,</span><br><span class="line">          jobGraph,</span><br><span class="line">          flinkConfiguration,</span><br><span class="line">          futureExecutor,</span><br><span class="line">          ioExecutor,</span><br><span class="line">          scheduler,</span><br><span class="line">          userCodeLoader,</span><br><span class="line">          checkpointRecoveryFactory,</span><br><span class="line">          Time.of(timeout.length, timeout.unit),</span><br><span class="line">          restartStrategy,</span><br><span class="line">          jobMetrics,</span><br><span class="line">          numSlots,</span><br><span class="line">          blobServer,</span><br><span class="line">          log.logger)</span><br><span class="line">          </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    if (leaderElectionService.hasLeadership) &#123;</span><br><span class="line">            log.info(s&quot;Scheduling job $jobId ($jobName).&quot;)</span><br><span class="line">            </span><br><span class="line">            executionGraph.scheduleForExecution()</span><br><span class="line">            </span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            self ! decorateMessage(RemoveJob(jobId, removeJobFromStateBackend = false))</span><br><span class="line"></span><br><span class="line">            log.warn(s&quot;Submitted job $jobId, but not leader. The other leader needs to recover &quot; +</span><br><span class="line">              &quot;this. I am not scheduling the job for execution.&quot;)</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先做一些准备工作，然后获取一个ExecutionGraph，判断是否是恢复的job，然后将job保存下来，并且通知客户端本地已经提交成功了，最后如果确认本JobManager是leader，则执行 <code>executionGraph.scheduleForExecution()</code> 方法，这个方法经过一系列调用，把每个ExecutionVertex传递给了Excution类的deploy方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public void deploy() throws JobException &#123;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            // good, we are allowed to deploy</span><br><span class="line">            if (!slot.setExecutedVertex(this)) &#123;</span><br><span class="line">                throw new JobException(&quot;Could not assign the ExecutionVertex to the slot &quot; + slot);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // race double check, did we fail/cancel and do we need to release the slot?</span><br><span class="line">            if (this.state != DEPLOYING) &#123;</span><br><span class="line">                slot.releaseSlot();</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (LOG.isInfoEnabled()) &#123;</span><br><span class="line">                LOG.info(String.format(&quot;Deploying %s (attempt #%d) to %s&quot;, vertex.getTaskNameWithSubtaskIndex(),</span><br><span class="line">                        attemptNumber, getAssignedResourceLocation().getHostname()));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            final TaskDeploymentDescriptor deployment = vertex.createDeploymentDescriptor(</span><br><span class="line">                attemptId,</span><br><span class="line">                slot,</span><br><span class="line">                taskState,</span><br><span class="line">                attemptNumber);</span><br><span class="line"></span><br><span class="line">            final TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();</span><br><span class="line"></span><br><span class="line">            final CompletableFuture&lt;Acknowledge&gt; submitResultFuture = taskManagerGateway.submitTask(deployment, timeout);</span><br><span class="line"></span><br><span class="line">            ......</span><br><span class="line">        &#125;</span><br><span class="line">        catch (Throwable t) &#123;</span><br><span class="line">            markFailed(t);</span><br><span class="line">            ExceptionUtils.rethrow(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>我们首先生成了一个TaskDeploymentDescriptor，然后交给了 <code>taskManagerGateway.submitTask()</code> 方法执行。接下来的部分，就属于TaskManager的范畴了。</p>
<h3 id="TaskManager执行task"><a href="#TaskManager执行task" class="headerlink" title="TaskManager执行task"></a>TaskManager执行task</h3><h4 id="TaskManager的基本组件"><a href="#TaskManager的基本组件" class="headerlink" title="TaskManager的基本组件"></a>TaskManager的基本组件</h4><p>TaskManager是flink中资源管理的基本组件，是所有执行任务的基本容器，提供了内存管理、IO管理、通信管理等一系列功能，本节对各个模块进行简要介绍。</p>
<ol>
<li>MemoryManager flink并没有把所有内存的管理都委托给JVM，因为JVM普遍存在着存储对象密度低、大内存时GC对系统影响大等问题。所以flink自己抽象了一套内存管理机制，将所有对象序列化后放在自己的MemorySegment上进行管理。MemoryManger涉及内容较多，将在后续章节进行继续剖析。</li>
<li>IOManager flink通过IOManager管理磁盘IO的过程，提供了同步和异步两种写模式，又进一步区分了block、buffer和bulk三种读写方式。<br>IOManager提供了两种方式枚举磁盘文件，一种是直接遍历文件夹下所有文件，另一种是计数器方式，对每个文件名以递增顺序访问。<br>在底层，flink将文件IO抽象为FileIOChannle，封装了底层实现。</li>
</ol>
<p><img src="/2018/09/18/flink-code-section-1/image_1cag7idg4vfj1l871n0l1k0e1f7u4o.png" alt=""></p>
<p>可以看到，flink在底层实际上都是以异步的方式进行读写。</p>
<ol start="3">
<li>NetworkEnvironment 是TaskManager的网络 IO 组件，包含了追踪中间结果和数据交换的数据结构。它的构造器会统一将配置的内存先分配出来，抽象成 NetworkBufferPool 统一管理内存的申请和释放。意思是说，在输入和输出数据时，不管是保留在本地内存，等待chain在一起的下个操作符进行处理，还是通过网络把本操作符的计算结果发送出去，都被抽象成了NetworkBufferPool。后续我们还将对这个组件进行详细分析。</li>
</ol>
<h4 id="TaskManager执行Task"><a href="#TaskManager执行Task" class="headerlink" title="TaskManager执行Task"></a>TaskManager执行Task</h4><p>对于TM来说，执行task就是把收到的 <code>TaskDeploymentDescriptor</code> 对象转换成一个task并执行的过程。TaskDeploymentDescriptor这个类保存了task执行所必须的所有内容，例如序列化的算子，输入的InputGate和输出的ResultPartition的定义，该task要作为几个subtask执行等等。<br>按照正常逻辑思维，很容易想到TM的submitTask方法的行为：首先是确认资源，如寻找JobManager和Blob，而后建立连接，解序列化算子，收集task相关信息，接下来就是创建一个新的 <code>Task</code> 对象，这个task对象就是真正执行任务的关键所在。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">val task = new Task(</span><br><span class="line">        jobInformation,</span><br><span class="line">        taskInformation,</span><br><span class="line">        tdd.getExecutionAttemptId,</span><br><span class="line">        tdd.getAllocationId,</span><br><span class="line">        tdd.getSubtaskIndex,</span><br><span class="line">        tdd.getAttemptNumber,</span><br><span class="line">        tdd.getProducedPartitions,</span><br><span class="line">        tdd.getInputGates,</span><br><span class="line">        tdd.getTargetSlotNumber,</span><br><span class="line">        tdd.getTaskStateHandles,</span><br><span class="line">        memoryManager,</span><br><span class="line">        ioManager,</span><br><span class="line">        network,</span><br><span class="line">        bcVarManager,</span><br><span class="line">        taskManagerConnection,</span><br><span class="line">        inputSplitProvider,</span><br><span class="line">        checkpointResponder,</span><br><span class="line">        blobCache,</span><br><span class="line">        libCache,</span><br><span class="line">        fileCache,</span><br><span class="line">        config,</span><br><span class="line">        taskMetricGroup,</span><br><span class="line">        resultPartitionConsumableNotifier,</span><br><span class="line">        partitionStateChecker,</span><br><span class="line">        context.dispatcher)</span><br></pre></td></tr></table></figure>
<p>如果读者是从头开始看这篇blog，里面有很多对象应该已经比较明确其作用了（除了那个brVarManager，这个是管理广播变量的，广播变量是一类会被分发到每个任务中的共享变量）。接下来的主要任务，就是把这个task启动起来,然后报告说已经启动task了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// all good, we kick off the task, which performs its own initialization</span><br><span class="line">task.startTaskThread()</span><br><span class="line"></span><br><span class="line">sender ! decorateMessage(Acknowledge.get())</span><br></pre></td></tr></table></figure>
<h4 id="3-3-2-1-生成Task对象"><a href="#3-3-2-1-生成Task对象" class="headerlink" title="3.3.2.1 生成Task对象"></a>3.3.2.1 生成Task对象</h4><p>在执行new Task()方法时，第一步是把构造函数里的这些变量赋值给当前task的fields。<br>接下来是初始化ResultPartition和InputGate。这两个类描述了task的输出数据和输入数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">for (ResultPartitionDeploymentDescriptor desc: resultPartitionDeploymentDescriptors) &#123;</span><br><span class="line">    ResultPartitionID partitionId = new ResultPartitionID(desc.getPartitionId(), executionId);</span><br><span class="line"></span><br><span class="line">    this.producedPartitions[counter] = new ResultPartition(</span><br><span class="line">        taskNameWithSubtaskAndId,</span><br><span class="line">        this,</span><br><span class="line">        jobId,</span><br><span class="line">        partitionId,</span><br><span class="line">        desc.getPartitionType(),</span><br><span class="line">        desc.getNumberOfSubpartitions(),</span><br><span class="line">        desc.getMaxParallelism(),</span><br><span class="line">        networkEnvironment.getResultPartitionManager(),</span><br><span class="line">        resultPartitionConsumableNotifier,</span><br><span class="line">        ioManager,</span><br><span class="line">        desc.sendScheduleOrUpdateConsumersMessage());        </span><br><span class="line">    //为每个partition初始化对应的writer </span><br><span class="line">    writers[counter] = new ResultPartitionWriter(producedPartitions[counter]);</span><br><span class="line"></span><br><span class="line">    ++counter;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Consumed intermediate result partitions</span><br><span class="line">this.inputGates = new SingleInputGate[inputGateDeploymentDescriptors.size()];</span><br><span class="line">this.inputGatesById = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">counter = 0;</span><br><span class="line"></span><br><span class="line">for (InputGateDeploymentDescriptor inputGateDeploymentDescriptor: inputGateDeploymentDescriptors) &#123;</span><br><span class="line">    SingleInputGate gate = SingleInputGate.create(</span><br><span class="line">        taskNameWithSubtaskAndId,</span><br><span class="line">        jobId,</span><br><span class="line">        executionId,</span><br><span class="line">        inputGateDeploymentDescriptor,</span><br><span class="line">        networkEnvironment,</span><br><span class="line">        this,</span><br><span class="line">        metricGroup.getIOMetricGroup());</span><br><span class="line"></span><br><span class="line">    inputGates[counter] = gate;</span><br><span class="line">    inputGatesById.put(gate.getConsumedResultId(), gate);</span><br><span class="line"></span><br><span class="line">    ++counter;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后，创建一个Thread对象，并把自己放进该对象，这样在执行时，自己就有了自身的线程的引用。</p>
<h4 id="运行Task对象"><a href="#运行Task对象" class="headerlink" title="运行Task对象"></a>运行Task对象</h4><p>Task对象本身就是一个Runable，因此在其run方法里定义了运行逻辑。<br>第一步是切换Task的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">while (true) &#123;</span><br><span class="line">            ExecutionState current = this.executionState;</span><br><span class="line">            ////如果当前的执行状态为CREATED，则将其设置为DEPLOYING状态</span><br><span class="line">            if (current == ExecutionState.CREATED) &#123;</span><br><span class="line">                if (transitionState(ExecutionState.CREATED, ExecutionState.DEPLOYING)) &#123;</span><br><span class="line">                    // success, we can start our work</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            //如果当前执行状态为FAILED，则发出通知并退出run方法</span><br><span class="line">            else if (current == ExecutionState.FAILED) &#123;</span><br><span class="line">                // we were immediately failed. tell the TaskManager that we reached our final state</span><br><span class="line">                notifyFinalState();</span><br><span class="line">                if (metrics != null) &#123;</span><br><span class="line">                    metrics.close();</span><br><span class="line">                &#125;</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">            //如果当前执行状态为CANCELING，则将其修改为CANCELED状态，并退出run</span><br><span class="line">            else if (current == ExecutionState.CANCELING) &#123;</span><br><span class="line">                if (transitionState(ExecutionState.CANCELING, ExecutionState.CANCELED)) &#123;</span><br><span class="line">                    // we were immediately canceled. tell the TaskManager that we reached our final state</span><br><span class="line">                    notifyFinalState();</span><br><span class="line">                    if (metrics != null) &#123;</span><br><span class="line">                        metrics.close();</span><br><span class="line">                    &#125;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            //否则说明发生了异常</span><br><span class="line">            else &#123;</span><br><span class="line">                if (metrics != null) &#123;</span><br><span class="line">                    metrics.close();</span><br><span class="line">                &#125;</span><br><span class="line">                throw new IllegalStateException(&quot;Invalid state for beginning of operation of task &quot; + this + &apos;.&apos;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>接下来，就是导入用户类加载器并加载用户代码。<br>然后，是向网络管理器注册当前任务（flink的各个算子在运行时进行数据交换需要依赖网络管理器），分配一些缓存以保存数据<br>然后，读入指定的缓存文件。<br>然后，再把task创建时传入的那一大堆变量用于创建一个执行环境Envrionment。<br>再然后，对于那些并不是第一次执行的task（比如失败后重启的）要恢复其状态。<br>接下来最重要的是</p>
<ol>
<li><code>invokable.invoke();</code><br>方法。为什么这么说呢，因为这个方法就是用户代码所真正被执行的入口。比如我们写的什么new MapFunction()的逻辑，最终就是在这里被执行的。这里说一下这个invokable，这是一个抽象类，提供了可以被TaskManager执行的对象的基本抽象。<br>这个invokable是在解析JobGraph的时候生成相关信息的，并在此处形成真正可执行的对象</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// now load the task&apos;s invokable code</span><br><span class="line">//通过反射生成对象</span><br><span class="line">invokable = loadAndInstantiateInvokable(userCodeClassLoader, nameOfInvokableClass);</span><br></pre></td></tr></table></figure>
<p><img src="/2018/09/18/flink-code-section-1/image_1cbkaa8r9182i18ct1kfu8g829m9.png" alt=""></p>
<p>上图显示了flink提供的可被执行的Task类型。从名字上就可以看出各个task的作用，在此不再赘述。<br>接下来就是invoke方法了，因为我们的wordcount例子用了流式api，在此我们以StreamTask的invoke方法为例进行说明。</p>
<h4 id="StreamTask的执行逻辑"><a href="#StreamTask的执行逻辑" class="headerlink" title="StreamTask的执行逻辑"></a>StreamTask的执行逻辑</h4><p>先上部分核心代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">public final void invoke() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">    boolean disposed = false;</span><br><span class="line">    try &#123;</span><br><span class="line">            // -------- Initialize ---------</span><br><span class="line">            //先做一些赋值操作</span><br><span class="line">            ......</span><br><span class="line"></span><br><span class="line">    // if the clock is not already set, then assign a default TimeServiceProvider</span><br><span class="line">    //处理timer</span><br><span class="line">    if (timerService == null) &#123;</span><br><span class="line">        ThreadFactory timerThreadFactory =</span><br><span class="line">            new DispatcherThreadFactory(TRIGGER_THREAD_GROUP, &quot;Time Trigger for &quot; + getName());</span><br><span class="line"></span><br><span class="line">        timerService = new SystemProcessingTimeService(this, getCheckpointLock(), timerThreadFactory);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //把之前JobGraph串起来的chain的信息形成实现</span><br><span class="line">    operatorChain = new OperatorChain&lt;&gt;(this);</span><br><span class="line">    headOperator = operatorChain.getHeadOperator();</span><br><span class="line"></span><br><span class="line">    // task specific initialization</span><br><span class="line">    //这个init操作的起名非常诡异，因为这里主要是处理算子采用了自定义的checkpoint检查机制的情况，但是起了一个非常大众脸的名字</span><br><span class="line">    init();</span><br><span class="line"></span><br><span class="line">    // save the work of reloading state, etc, if the task is already canceled</span><br><span class="line">    if (canceled) &#123;</span><br><span class="line">        throw new CancelTaskException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // -------- Invoke --------</span><br><span class="line">    LOG.debug(&quot;Invoking &#123;&#125;&quot;, getName());</span><br><span class="line">            </span><br><span class="line">    // we need to make sure that any triggers scheduled in open() cannot be</span><br><span class="line">    // executed before all operators are opened</span><br><span class="line">    synchronized (lock) &#123;</span><br><span class="line"></span><br><span class="line">        // both the following operations are protected by the lock</span><br><span class="line">        // so that we avoid race conditions in the case that initializeState()</span><br><span class="line">        // registers a timer, that fires before the open() is called.</span><br><span class="line"></span><br><span class="line">        //初始化操作符状态，主要是一些state啥的</span><br><span class="line">        initializeState();</span><br><span class="line">        //对于富操作符，执行其open操作</span><br><span class="line">        openAllOperators();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // final check to exit early before starting to run</span><br><span class="line">    f (canceled) &#123;</span><br><span class="line">        throw new CancelTaskException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // let the task do its work</span><br><span class="line">    //真正开始执行的代码</span><br><span class="line">    isRunning = true;</span><br><span class="line">    run();</span><br></pre></td></tr></table></figure>
<p>StreamTask.invoke()方法里，第一个值得一说的是 <code>TimerService</code> 。Flink在2015年决定向StreamTask类加入timer service的时候解释到：</p>
<blockquote>
<p>This integrates the timer as a service in StreamTask that StreamOperators can use by calling a method on the StreamingRuntimeContext. This also ensures that the timer callbacks can not be called concurrently with other methods on the StreamOperator. This behaviour is ensured by an ITCase.  </p>
</blockquote>
<p>第二个要注意的是chain操作。前面提到了，flink会出于优化的角度，把一些算子chain成一个整体的算子作为一个task来执行。比如wordcount例子中，Source和FlatMap算子就被chain在了一起。在进行chain操作的时候，会设定头节点，并且指定输出的RecordWriter。</p>
<p>接下来不出所料仍然是初始化，只不过初始化的对象变成了各个operator。如果是有checkpoint的，那就从state信息里恢复，不然就作为全新的算子处理。从源码中可以看到，flink针对keyed算子和普通算子做了不同的处理。keyed算子在初始化时需要计算出一个group区间，这个区间的值在整个生命周期里都不会再变化，后面key就会根据hash的不同结果，分配到特定的group中去计算。顺便提一句，flink的keyed算子保存的是对每个数据的key的计算方法，而非真实的key，用户需要自己保证对每一行数据提供的keySelector的幂等性。至于为什么要用KeyGroup的设计，这就牵扯到扩容的范畴了，将在后面的章节进行讲述。<br>对于 <code>openAllOperators()</code> 方法，就是对各种RichOperator执行其open方法，通常可用于在执行计算之前加载资源。<br>最后，run方法千呼万唤始出来，该方法经过一系列跳转，最终调用chain上的第一个算子的run方法。在wordcount的例子中，它最终调用了SocketTextStreamFunction的run，建立socket连接并读入文本。</p>
<h3 id="StreamTask与StreamOperator"><a href="#StreamTask与StreamOperator" class="headerlink" title="StreamTask与StreamOperator"></a>StreamTask与StreamOperator</h3><p>前面提到，Task对象在执行过程中，把执行的任务交给了StreamTask这个类去执行。在我们的wordcount例子中，实际初始化的是OneInputStreamTask的对象（参考上面的类图）。那么这个对象是如何执行用户的代码的呢？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">protected void run() throws Exception &#123;</span><br><span class="line">        // cache processor reference on the stack, to make the code more JIT friendly</span><br><span class="line">        final StreamInputProcessor&lt;IN&gt; inputProcessor = this.inputProcessor;</span><br><span class="line"></span><br><span class="line">        while (running &amp;&amp; inputProcessor.processInput()) &#123;</span><br><span class="line">            // all the work happens in the &quot;processInput&quot; method</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>它做的，就是把任务直接交给了InputProcessor去执行processInput方法。这是一个 <code>StreamInputProcessor</code> 的实例，该processor的任务就是处理输入的数据，包括用户数据、watermark和checkpoint数据等。我们先来看看这个processor是如何产生的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public void init() throws Exception &#123;</span><br><span class="line">        StreamConfig configuration = getConfiguration();</span><br><span class="line"></span><br><span class="line">        TypeSerializer&lt;IN&gt; inSerializer = configuration.getTypeSerializerIn1(getUserCodeClassLoader());</span><br><span class="line">        int numberOfInputs = configuration.getNumberOfInputs();</span><br><span class="line"></span><br><span class="line">        if (numberOfInputs &gt; 0) &#123;</span><br><span class="line">            InputGate[] inputGates = getEnvironment().getAllInputGates();</span><br><span class="line"></span><br><span class="line">            inputProcessor = new StreamInputProcessor&lt;&gt;(</span><br><span class="line">                    inputGates,</span><br><span class="line">                    inSerializer,</span><br><span class="line">                    this,</span><br><span class="line">                    configuration.getCheckpointMode(),</span><br><span class="line">                    getCheckpointLock(),</span><br><span class="line">                    getEnvironment().getIOManager(),</span><br><span class="line">                    getEnvironment().getTaskManagerInfo().getConfiguration(),</span><br><span class="line">                    getStreamStatusMaintainer(),</span><br><span class="line">                    this.headOperator);</span><br><span class="line"></span><br><span class="line">            // make sure that stream tasks report their I/O statistics</span><br><span class="line">            inputProcessor.setMetricGroup(getEnvironment().getMetricGroup().getIOMetricGroup());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这是OneInputStreamTask的init方法，从configs里面获取StreamOperator信息，生成自己的inputProcessor。那么inputProcessor是如何处理数据的呢？我们接着跟进源码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">public boolean processInput() throws Exception &#123;</span><br><span class="line">        if (isFinished) &#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line">        if (numRecordsIn == null) &#123;</span><br><span class="line">            numRecordsIn = ((OperatorMetricGroup) streamOperator.getMetricGroup()).getIOMetricGroup().getNumRecordsInCounter();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        //这个while是用来处理单个元素的（不要想当然以为是循环处理元素的）</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            //注意 1在下面</span><br><span class="line">            //2.接下来，会利用这个反序列化器得到下一个数据记录，并进行解析（是用户数据还是watermark等等），然后进行对应的操作</span><br><span class="line">            if (currentRecordDeserializer != null) &#123;</span><br><span class="line">                DeserializationResult result = currentRecordDeserializer.getNextRecord(deserializationDelegate);</span><br><span class="line"></span><br><span class="line">                if (result.isBufferConsumed()) &#123;</span><br><span class="line">                    currentRecordDeserializer.getCurrentBuffer().recycle();</span><br><span class="line">                    currentRecordDeserializer = null;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                if (result.isFullRecord()) &#123;</span><br><span class="line">                    StreamElement recordOrMark = deserializationDelegate.getInstance();</span><br><span class="line"></span><br><span class="line">                    //如果元素是watermark，就准备更新当前channel的watermark值（并不是简单赋值，因为有乱序存在），</span><br><span class="line">                    if (recordOrMark.isWatermark()) &#123;</span><br><span class="line">                        // handle watermark</span><br><span class="line">                        statusWatermarkValve.inputWatermark(recordOrMark.asWatermark(), currentChannel);</span><br><span class="line">                        continue;</span><br><span class="line">                    &#125; else if (recordOrMark.isStreamStatus()) &#123;</span><br><span class="line">                    //如果元素是status，就进行相应处理。可以看作是一个flag，标志着当前stream接下来即将没有元素输入（idle），或者当前即将由空闲状态转为有元素状态（active）。同时，StreamStatus还对如何处理watermark有影响。通过发送status，上游的operator可以很方便的通知下游当前的数据流的状态。</span><br><span class="line">                        // handle stream status</span><br><span class="line">                        statusWatermarkValve.inputStreamStatus(recordOrMark.asStreamStatus(), currentChannel);</span><br><span class="line">                        continue;</span><br><span class="line">                    &#125; else if (recordOrMark.isLatencyMarker()) &#123;</span><br><span class="line">                    //LatencyMarker是用来衡量代码执行时间的。在Source处创建，携带创建时的时间戳，流到Sink时就可以知道经过了多长时间</span><br><span class="line">                        // handle latency marker</span><br><span class="line">                        synchronized (lock) &#123;</span><br><span class="line">                            streamOperator.processLatencyMarker(recordOrMark.asLatencyMarker());</span><br><span class="line">                        &#125;</span><br><span class="line">                        continue;</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                    //这里就是真正的，用户的代码即将被执行的地方。从章节1到这里足足用了三万字，有点万里长征的感觉</span><br><span class="line">                        // now we can do the actual processing</span><br><span class="line">                        StreamRecord&lt;IN&gt; record = recordOrMark.asRecord();</span><br><span class="line">                        synchronized (lock) &#123;</span><br><span class="line">                            numRecordsIn.inc();</span><br><span class="line">                            streamOperator.setKeyContextElement1(record);</span><br><span class="line">                            streamOperator.processElement(record);</span><br><span class="line">                        &#125;</span><br><span class="line">                        return true;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            //1.程序首先获取下一个buffer</span><br><span class="line">            //这一段代码是服务于flink的FaultTorrent机制的，后面我会讲到，这里只需理解到它会尝试获取buffer，然后赋值给当前的反序列化器</span><br><span class="line">            final BufferOrEvent bufferOrEvent = barrierHandler.getNextNonBlocked();</span><br><span class="line">            if (bufferOrEvent != null) &#123;</span><br><span class="line">                if (bufferOrEvent.isBuffer()) &#123;</span><br><span class="line">                    currentChannel = bufferOrEvent.getChannelIndex();</span><br><span class="line">                    currentRecordDeserializer = recordDeserializers[currentChannel];</span><br><span class="line">                    currentRecordDeserializer.setNextBuffer(bufferOrEvent.getBuffer());</span><br><span class="line">                &#125;</span><br><span class="line">                else &#123;</span><br><span class="line">                    // Event received</span><br><span class="line">                    final AbstractEvent event = bufferOrEvent.getEvent();</span><br><span class="line">                    if (event.getClass() != EndOfPartitionEvent.class) &#123;</span><br><span class="line">                        throw new IOException(&quot;Unexpected event: &quot; + event);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                isFinished = true;</span><br><span class="line">                if (!barrierHandler.isEmpty()) &#123;</span><br><span class="line">                    throw new IllegalStateException(&quot;Trailing data in checkpoint barrier handler.&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">                return false;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>到此为止，以上部分就是一个flink程序启动后，到执行用户代码之前，flink框架所做的准备工作。回顾一下：</p>
<ul>
<li>启动一个环境</li>
<li>生成StreamGraph</li>
<li>注册和选举JobManager</li>
<li>在各节点生成TaskManager，并根据JobGraph生成对应的Task</li>
<li>启动各个task，准备执行代码<br>接下来，我们挑几个Operator看看flink是如何抽象这些算子的。</li>
</ul>
<h2 id="StreamOperator的抽象与实现"><a href="#StreamOperator的抽象与实现" class="headerlink" title="StreamOperator的抽象与实现"></a>StreamOperator的抽象与实现</h2><h3 id="数据源的逻辑——StreamSource与时间模型"><a href="#数据源的逻辑——StreamSource与时间模型" class="headerlink" title="数据源的逻辑——StreamSource与时间模型"></a>数据源的逻辑——StreamSource与时间模型</h3><p>StreamSource抽象了一个数据源，并且指定了一些如何处理数据的模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">public class StreamSource&lt;OUT, SRC extends SourceFunction&lt;OUT&gt;&gt;</span><br><span class="line">        extends AbstractUdfStreamOperator&lt;OUT, SRC&gt; implements StreamOperator&lt;OUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    public void run(final Object lockingObject, final StreamStatusMaintainer streamStatusMaintainer) throws Exception &#123;</span><br><span class="line">        run(lockingObject, streamStatusMaintainer, output);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void run(final Object lockingObject,</span><br><span class="line">            final StreamStatusMaintainer streamStatusMaintainer,</span><br><span class="line">            final Output&lt;StreamRecord&lt;OUT&gt;&gt; collector) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        final TimeCharacteristic timeCharacteristic = getOperatorConfig().getTimeCharacteristic();</span><br><span class="line"></span><br><span class="line">        LatencyMarksEmitter latencyEmitter = null;</span><br><span class="line">        if (getExecutionConfig().isLatencyTrackingEnabled()) &#123;</span><br><span class="line">            latencyEmitter = new LatencyMarksEmitter&lt;&gt;(</span><br><span class="line">                getProcessingTimeService(),</span><br><span class="line">                collector,</span><br><span class="line">                getExecutionConfig().getLatencyTrackingInterval(),</span><br><span class="line">                getOperatorConfig().getVertexID(),</span><br><span class="line">                getRuntimeContext().getIndexOfThisSubtask());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        final long watermarkInterval = getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval();</span><br><span class="line"></span><br><span class="line">        this.ctx = StreamSourceContexts.getSourceContext(</span><br><span class="line">            timeCharacteristic,</span><br><span class="line">            getProcessingTimeService(),</span><br><span class="line">            lockingObject,</span><br><span class="line">            streamStatusMaintainer,</span><br><span class="line">            collector,</span><br><span class="line">            watermarkInterval,</span><br><span class="line">            -1);</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            userFunction.run(ctx);</span><br><span class="line"></span><br><span class="line">            // if we get here, then the user function either exited after being done (finite source)</span><br><span class="line">            // or the function was canceled or stopped. For the finite source case, we should emit</span><br><span class="line">            // a final watermark that indicates that we reached the end of event-time</span><br><span class="line">            if (!isCanceledOrStopped()) &#123;</span><br><span class="line">                ctx.emitWatermark(Watermark.MAX_WATERMARK);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            // make sure that the context is closed in any case</span><br><span class="line">            ctx.close();</span><br><span class="line">            if (latencyEmitter != null) &#123;</span><br><span class="line">                latencyEmitter.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    private static class LatencyMarksEmitter&lt;OUT&gt; &#123;</span><br><span class="line">        private final ScheduledFuture&lt;?&gt; latencyMarkTimer;</span><br><span class="line"></span><br><span class="line">        public LatencyMarksEmitter(</span><br><span class="line">                final ProcessingTimeService processingTimeService,</span><br><span class="line">                final Output&lt;StreamRecord&lt;OUT&gt;&gt; output,</span><br><span class="line">                long latencyTrackingInterval,</span><br><span class="line">                final int vertexID,</span><br><span class="line">                final int subtaskIndex) &#123;</span><br><span class="line"></span><br><span class="line">            latencyMarkTimer = processingTimeService.scheduleAtFixedRate(</span><br><span class="line">                new ProcessingTimeCallback() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public void onProcessingTime(long timestamp) throws Exception &#123;</span><br><span class="line">                        try &#123;</span><br><span class="line">                            // ProcessingTimeService callbacks are executed under the checkpointing lock</span><br><span class="line">                            output.emitLatencyMarker(new LatencyMarker(timestamp, vertexID, subtaskIndex));</span><br><span class="line">                        &#125; catch (Throwable t) &#123;</span><br><span class="line">                            // we catch the Throwables here so that we don&apos;t trigger the processing</span><br><span class="line">                            // timer services async exception handler</span><br><span class="line">                            LOG.warn(&quot;Error while emitting latency marker.&quot;, t);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                0L,</span><br><span class="line">                latencyTrackingInterval);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        public void close() &#123;</span><br><span class="line">            latencyMarkTimer.cancel(true);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在StreamSource生成上下文之后，接下来就是把上下文交给SourceFunction去执行:</p>
<ol>
<li><code>userFunction.run(ctx);</code><br>SourceFunction是对Function的一个抽象，就好像MapFunction，KeyByFunction一样，用户选择实现这些函数，然后flink框架就能利用这些函数进行计算，完成用户逻辑。<br>我们的wordcount程序使用了flink提供的一个 <code>SocketTextStreamFunction</code> 。我们可以看一下它的实现逻辑，对source如何运行有一个基本的认识：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123;</span><br><span class="line">        final StringBuilder buffer = new StringBuilder();</span><br><span class="line">        long attempt = 0;</span><br><span class="line"></span><br><span class="line">        while (isRunning) &#123;</span><br><span class="line"></span><br><span class="line">            try (Socket socket = new Socket()) &#123;</span><br><span class="line">                currentSocket = socket;</span><br><span class="line"></span><br><span class="line">                LOG.info(&quot;Connecting to server socket &quot; + hostname + &apos;:&apos; + port);</span><br><span class="line">                socket.connect(new InetSocketAddress(hostname, port), CONNECTION_TIMEOUT_TIME);</span><br><span class="line">                BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()));</span><br><span class="line"></span><br><span class="line">                char[] cbuf = new char[8192];</span><br><span class="line">                int bytesRead;</span><br><span class="line">                //核心逻辑就是一直读inputSocket,然后交给collect方法</span><br><span class="line">                while (isRunning &amp;&amp; (bytesRead = reader.read(cbuf)) != -1) &#123;</span><br><span class="line">                    buffer.append(cbuf, 0, bytesRead);</span><br><span class="line">                    int delimPos;</span><br><span class="line">                    while (buffer.length() &gt;= delimiter.length() &amp;&amp; (delimPos = buffer.indexOf(delimiter)) != -1) &#123;</span><br><span class="line">                        String record = buffer.substring(0, delimPos);</span><br><span class="line">                        // truncate trailing carriage return</span><br><span class="line">                        if (delimiter.equals(&quot;\n&quot;) &amp;&amp; record.endsWith(&quot;\r&quot;)) &#123;</span><br><span class="line">                            record = record.substring(0, record.length() - 1);</span><br><span class="line">                        &#125;</span><br><span class="line">                        //读到数据后，把数据交给collect方法，collect方法负责把数据交到合适的位置（如发布为br变量，或者交给下个operator，或者通过网络发出去）</span><br><span class="line">                        ctx.collect(record);</span><br><span class="line">                        buffer.delete(0, delimPos + delimiter.length());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // if we dropped out of this loop due to an EOF, sleep and retry</span><br><span class="line">            if (isRunning) &#123;</span><br><span class="line">                attempt++;</span><br><span class="line">                if (maxNumRetries == -1 || attempt &lt; maxNumRetries) &#123;</span><br><span class="line">                    LOG.warn(&quot;Lost connection to server socket. Retrying in &quot; + delayBetweenRetries + &quot; msecs...&quot;);</span><br><span class="line">                    Thread.sleep(delayBetweenRetries);</span><br><span class="line">                &#125;</span><br><span class="line">                else &#123;</span><br><span class="line">                    // this should probably be here, but some examples expect simple exists of the stream source</span><br><span class="line">                    // throw new EOFException(&quot;Reached end of stream and reconnects are not enabled.&quot;);</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // collect trailing data</span><br><span class="line">        if (buffer.length() &gt; 0) &#123;</span><br><span class="line">            ctx.collect(buffer.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>整段代码里，只有collect方法有些复杂度，后面我们在讲到flink的对象机制时会结合来讲，此处知道collect方法会收集结果，然后发送给接收者即可。在我们的wordcount里，这个算子的接收者就是被chain在一起的flatmap算子，不记得这个示例程序的话，可以返回第一章去看一下。</p>
<h3 id="从数据输入到数据处理——OneInputStreamOperator-amp-AbstractUdfStreamOperator"><a href="#从数据输入到数据处理——OneInputStreamOperator-amp-AbstractUdfStreamOperator" class="headerlink" title="从数据输入到数据处理——OneInputStreamOperator &amp; AbstractUdfStreamOperator"></a>从数据输入到数据处理——OneInputStreamOperator &amp; AbstractUdfStreamOperator</h3><p>StreamSource是用来开启整个流的算子，而承接输入数据并进行处理的算子就是OneInputStreamOperator、TwoInputStreamOperator等。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cdc1tbgs136k1ppf17at14fumjf2d.png" alt=""></p>
<p>整个StreamOperator的继承关系如上图所示（图很大，建议点开放大看）。<br>OneInputStreamOperator这个接口的逻辑很简单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public interface OneInputStreamOperator&lt;IN, OUT&gt; extends StreamOperator&lt;OUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Processes one element that arrived at this operator.</span><br><span class="line">     * This method is guaranteed to not be called concurrently with other methods of the operator.</span><br><span class="line">     */</span><br><span class="line">    void processElement(StreamRecord&lt;IN&gt; element) throws Exception;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Processes a &#123;@link Watermark&#125;.</span><br><span class="line">     * This method is guaranteed to not be called concurrently with other methods of the operator.</span><br><span class="line">     *</span><br><span class="line">     * @see org.apache.flink.streaming.api.watermark.Watermark</span><br><span class="line">     */</span><br><span class="line">    void processWatermark(Watermark mark) throws Exception;</span><br><span class="line"></span><br><span class="line">    void processLatencyMarker(LatencyMarker latencyMarker) throws Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而实现了这个接口的StreamFlatMap算子也很简单，没什么可说的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public class StreamFlatMap&lt;IN, OUT&gt;</span><br><span class="line">        extends AbstractUdfStreamOperator&lt;OUT, FlatMapFunction&lt;IN, OUT&gt;&gt;</span><br><span class="line">        implements OneInputStreamOperator&lt;IN, OUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">    private transient TimestampedCollector&lt;OUT&gt; collector;</span><br><span class="line"></span><br><span class="line">    public StreamFlatMap(FlatMapFunction&lt;IN, OUT&gt; flatMapper) &#123;</span><br><span class="line">        super(flatMapper);</span><br><span class="line">        chainingStrategy = ChainingStrategy.ALWAYS;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void open() throws Exception &#123;</span><br><span class="line">        super.open();</span><br><span class="line">        collector = new TimestampedCollector&lt;&gt;(output);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123;</span><br><span class="line">        collector.setTimestamp(element);</span><br><span class="line">        userFunction.flatMap(element.getValue(), collector);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从类图里可以看到，flink为我们封装了一个算子的基类 <code>AbstractUdfStreamOperator</code> ，提供了一些通用功能，比如把context赋给算子，保存快照等等，其中最为大家了解的应该是这两个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">    public void open() throws Exception &#123;</span><br><span class="line">        super.open();</span><br><span class="line">        FunctionUtils.openFunction(userFunction, new Configuration());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        super.close();</span><br><span class="line">        functionsClosed = true;</span><br><span class="line">        FunctionUtils.closeFunction(userFunction);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这两个就是flink提供的 <code>Rich***Function</code> 系列算子的open和close方法被执行的地方。</p>
<h3 id="StreamSink"><a href="#StreamSink" class="headerlink" title="StreamSink"></a>StreamSink</h3><p>StreamSink着实没什么可说的，逻辑很简单，值得一提的只有两个方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">    public void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123;</span><br><span class="line">        sinkContext.element = element;</span><br><span class="line">        userFunction.invoke(element.getValue(), sinkContext);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reportOrForwardLatencyMarker(LatencyMarker maker) &#123;</span><br><span class="line">        // all operators are tracking latencies</span><br><span class="line">        this.latencyGauge.reportLatency(maker, true);</span><br><span class="line"></span><br><span class="line">        // sinks don&apos;t forward latency markers</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>其中， <code>processElement</code> 是继承自StreamOperator的方法。 <code>reportOrForwardLatencyMarker</code> 是用来计算延迟的，前面提到StreamSource会产生LateMarker，用于记录数据计算时间，就是在这里完成了计算。</p>
<p>算子这部分逻辑相对简单清晰，就讲这么多吧。</p>
<h2 id="为执行保驾护航——Fault-Tolerant与保证Exactly-Once语义"><a href="#为执行保驾护航——Fault-Tolerant与保证Exactly-Once语义" class="headerlink" title="为执行保驾护航——Fault Tolerant与保证Exactly-Once语义"></a>为执行保驾护航——Fault Tolerant与保证Exactly-Once语义</h2><h3 id="Fault-Tolerant演进之路"><a href="#Fault-Tolerant演进之路" class="headerlink" title="Fault Tolerant演进之路"></a>Fault Tolerant演进之路</h3><p>对于7×24小时不间断运行的流程序来说，要保证fault tolerant是很难的，这不像是离线任务，如果失败了只需要清空已有结果，重新跑一次就可以了。对于流任务，如果要保证能够重新处理已处理过的数据，就要把数据保存下来；而这就面临着几个问题：比如一是保存多久的数据？二是重复计算的数据应该怎么处理，怎么保证幂等性？<br>对于一个流系统，我们有以下希望：</p>
<ol>
<li>最好能做到exactly-once</li>
<li>处理延迟越低越好</li>
<li>吞吐量越高越好</li>
<li>计算模型应当足够简单易用，又具有足够的表达力</li>
<li>从错误恢复的开销越低越好</li>
<li>足够的流控制能力（背压能力）</li>
</ol>
<h4 id="Storm的Record-acknowledgement模式"><a href="#Storm的Record-acknowledgement模式" class="headerlink" title="Storm的Record acknowledgement模式"></a>Storm的Record acknowledgement模式</h4><p>storm的fault tolerant是这样工作的：每一个被storm的operator处理的数据都会向其上一个operator发送一份应答消息，通知其已被下游处理。storm的源operator保存了所有已发送的消息的每一个下游算子的应答消息，当它收到来自sink的应答时，它就知道该消息已经被完整处理，可以移除了。<br>如果没有收到应答，storm就会重发该消息。显而易见，这是一种at least once的逻辑。另外，这种方式面临着严重的幂等性问题，例如对一个count算子，如果count的下游算子出错，source重发该消息，那么防止该消息被count两遍的逻辑需要程序员自己去实现。最后，这样一种处理方式非常低效，吞吐量很低。</p>
<h4 id="Spark-streaming的micro-batch模式"><a href="#Spark-streaming的micro-batch模式" class="headerlink" title="Spark streaming的micro batch模式"></a>Spark streaming的micro batch模式</h4><p>前面提到，storm的实现方式就注定了与高吞吐量无缘。那么，为了提高吞吐量，把一批数据聚集在一起处理就是很自然的选择。Spark Streaming的实现就是基于这样的思路：<br>我们可以在完全的连续计算与完全的分批计算中间取折中，通过控制每批计算数据的大小来控制延迟与吞吐量的制约，如果想要低延迟，就用小一点的batch，如果想要大吞吐量，就不得不忍受更高的延迟（更久的等待数据到来的时间和更多的计算），如下图所示。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceop58ha180p1h3ren58jk15gb9.png" alt=""></p>
<p>以这样的方式，可以在每个batch中做到exactly-once，但是这种方式也有其弊端：<br>首先，batch的方式使得一些需要跨batch的操作变得非常困难，例如session window；用户不得不自己想办法去实现相关逻辑。<br>其次，batch模式很难做好背压。当一个batch因为种种原因处理慢了，那么下一个batch要么不得不容纳更多的新来数据，要么不得不堆积更多的batch，整个任务可能会被拖垮，这是一个非常致命的问题。<br>最后，batch的方式基本意味着其延迟是有比较高的下限的，实时性上不好。</p>
<h4 id="Google-Cloud-Dataflow的事务式模型"><a href="#Google-Cloud-Dataflow的事务式模型" class="headerlink" title="Google Cloud Dataflow的事务式模型"></a>Google Cloud Dataflow的事务式模型</h4><p>我们在传统数据库，如mysql中使用binlog来完成事务，这样的思路也可以被用在实现exactly-once模型中。例如，我们可以log下每个数据元素每一次被处理时的结果和当时所处的操作符的状态。这样，当我们需要fault tolerant时，我们只需要读一下log就可以了。这种模式规避了storm和spark所面临的问题，并且能够很好的实现exactly-once，唯一的弊端是：如何尽可能的减少log的成本？Flink给了我们答案。</p>
<h4 id="Flink的分布式快照机制"><a href="#Flink的分布式快照机制" class="headerlink" title="Flink的分布式快照机制"></a>Flink的分布式快照机制</h4><p>实现exactly-once的关键是什么？是能够准确的知道和快速记录下来当前的operator的状态、当前正在处理的元素（以及正处在不同算子之间传递的元素）。如果上面这些可以做到，那么fault tolerant无非就是从持久化存储中读取上次记录的这些元信息，并且恢复到程序中。那么Flink是如何实现的呢？</p>
<p>Flink的分布式快照的核心是其轻量级异步分布式快照机制。为了实现这一机制，flink引入了一个概念，叫做Barrier。Barrier是一种标记，它被source产生并且插入到流数据中，被发送到下游节点。当下游节点处理到该barrier标志时，这就意味着在该barrier插入到流数据时，已经进入系统的数据在当前节点已经被处理完毕。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceos05badva20hb5glen1voqm.png" alt=""><br>如图所示，每当一个barrier流过一个算子节点时，就说明了在该算子上，可以触发一次检查点，用以保存当前节点的状态和已经处理过的数据，这就是一份快照。（在这里可以联想一下micro-batch，把barrier想象成分割每个batch的逻辑，会好理解一点）这样的方式下，记录快照就像和前面提到的micro-batch一样容易。</p>
<p>与此同时，该算子会向下游发送该barrier。因为数据在算子之间是按顺序发送的，所以当下游节点收到该barrier时，也就意味着同样的一批数据在下游节点上也处理完毕，可以进行一次checkpoint，保存基于该节点的一份快照，快照完成后，会通知JobMananger自己完成了这个快照。这就是分布式快照的基本含义。</p>
<p>再看这张图：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceot7q13apu1a04170af7j1jao34.png" alt=""></p>
<p>有时，有的算子的上游节点和下游节点都不止一个，应该怎么处理呢？如果有不止一个下游节点，就向每个下游发送barrier。同理，如果有不止一个上游节点，那么就要等到所有上游节点的同一批次的barrier到达之后，才能触发checkpoint。因为每个节点运算速度不同，所以有的上游节点可能已经在发下个barrier周期的数据了，有的上游节点还没发送本次的barrier，这时候，当前算子就要缓存一下提前到来的数据，等比较慢的上游节点发送barrier之后，才能处理下一批数据。</p>
<p>当整个程序的最后一个算子sink都收到了这个barrier，也就意味着这个barrier和上个barrier之间所夹杂的这批元素已经全部落袋为安。这时，最后一个算子通知JobManager整个流程已经完成，而JobManager随后发出通知，要求所有算子删除本次快照内容，以完成清理。这整个部分，就是Flink的 <strong>两阶段提交的checkpoint过程</strong> ，如下面四幅图所示： </p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceot517e14g31u2u1mnt12o91dkb1g.png" alt=""></p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceot5kqbnik1f2i1dss1q5c1a1t.png" alt=""></p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceot64dppjtojkq3n1jl5j0h2a.png" alt=""></p>
<p><img src="/2018/09/18/flink-code-section-1/image_1ceot6kes56sidn1f2u1voo19kf2n.png" alt=""><br>总之，通过这种方式，flink实现了我们前面提到的六项对流处理框架的要求：exactly-once、低延迟、高吞吐、易用的模型、方便的恢复机制。</p>
<p>最后，贴一个美团做的flink与storm的性能对比： <a href="https://tech.meituan.com/Flink_Benchmark.html" target="_blank" rel="noopener">flink与storm的性能对比</a></p>
<h3 id="checkpoint的生命周期"><a href="#checkpoint的生命周期" class="headerlink" title="checkpoint的生命周期"></a>checkpoint的生命周期</h3><p>接下来，我们结合源码来看看flink的checkpoint到底是如何实现其生命周期的：</p>
<blockquote>
<p>由于flink提供的SocketSource并不支持checkpoint，所以这里我以 <code>FlinkKafkaConsumer010</code> 作为sourceFunction。  </p>
</blockquote>
<h4 id="触发checkpoint"><a href="#触发checkpoint" class="headerlink" title="触发checkpoint"></a>触发checkpoint</h4><p>要完成一次checkpoint，第一步必然是发起checkpoint请求。那么，这个请求是哪里发出的，怎么发出的，又由谁控制呢？<br>还记得如果我们要设置checkpoint的话，需要指定checkpoint间隔吧？既然是一个指定间隔触发的功能，那应该会有类似于Scheduler的东西存在，flink里，这个负责触发checkpoint的类是 <code>CheckpointCoordinator</code> 。</p>
<p>flink在提交job时，会启动这个类的 <code>startCheckpointScheduler</code> 方法，如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public void startCheckpointScheduler() &#123;</span><br><span class="line">        synchronized (lock) &#123;</span><br><span class="line">            if (shutdown) &#123;</span><br><span class="line">                throw new IllegalArgumentException(&quot;Checkpoint coordinator is shut down&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // make sure all prior timers are cancelled</span><br><span class="line">            stopCheckpointScheduler();</span><br><span class="line"></span><br><span class="line">            periodicScheduling = true;</span><br><span class="line">            currentPeriodicTrigger = timer.scheduleAtFixedRate(</span><br><span class="line">                    new ScheduledTrigger(), </span><br><span class="line">                    baseInterval, baseInterval, TimeUnit.MILLISECONDS);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    private final class ScheduledTrigger implements Runnable &#123;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void run() &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                triggerCheckpoint(System.currentTimeMillis(), true);</span><br><span class="line">            &#125;</span><br><span class="line">            catch (Exception e) &#123;</span><br><span class="line">                LOG.error(&quot;Exception while triggering checkpoint.&quot;, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>启动之后，就会以设定好的频率调用 <code>triggerCheckPoint()</code> 方法。这个方法太长，我大概说一下都做了什么：</p>
<ul>
<li>检查符合触发checkpoint的条件，例如如果禁止了周期性的checkpoint，尚未达到触发checkpoint的最小间隔等等，就直接return</li>
<li>检查是否所有需要checkpoint和需要响应checkpoint的ACK（ack涉及到checkpoint的两阶段提交，后面会讲）的task都处于running状态，否则return</li>
<li>如果都符合，那么执行 <code>checkpointID = checkpointIdCounter.getAndIncrement();</code> 以生成一个新的id，然后生成一个 <code>PendingCheckpoint</code> 。PendingCheckpoint是一个启动了的checkpoint，但是还没有被确认。等到所有的task都确认了本次checkpoint，那么这个checkpoint对象将转化为一个 <code>CompletedCheckpoint</code> 。</li>
<li>定义一个超时callback，如果checkpoint执行了很久还没完成，就把它取消</li>
<li>触发MasterHooks，用户可以定义一些额外的操作，用以增强checkpoint的功能（如准备和清理外部资源）</li>
<li>接下来是核心逻辑：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// send the messages to the tasks that trigger their checkpoint</span><br><span class="line">    for (Execution execution: executions) &#123;</span><br><span class="line">        execution.triggerCheckpoint(checkpointID, timestamp, checkpointOptions);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里是调用了Execution的triggerCheckpoint方法，一个execution就是一个executionVertex的实际执行者。我们看一下这个方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public void triggerCheckpoint(long checkpointId, long timestamp, CheckpointOptions checkpointOptions) &#123;</span><br><span class="line">        final LogicalSlot slot = assignedResource;</span><br><span class="line"></span><br><span class="line">        if (slot != null) &#123;</span><br><span class="line">        //TaskManagerGateway是用来跟taskManager进行通信的组件</span><br><span class="line">            final TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();</span><br><span class="line"></span><br><span class="line">            taskManagerGateway.triggerCheckpoint(attemptId, getVertex().getJobId(), checkpointId, timestamp, checkpointOptions);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            LOG.debug(&quot;The execution has no slot assigned. This indicates that the execution is &quot; +</span><br><span class="line">                &quot;no longer running.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>再往下跟就进入了 <code>Task</code> 类的范畴，我们将在下一小节进行解读。本小节主要讲了 <code>CheckpointCoordinator</code> 类是如何触发一次checkpoint，从其名字也可以看出来其功能：检查点协调器。</p>
<h4 id="Task层面checkpoint的准备工作"><a href="#Task层面checkpoint的准备工作" class="headerlink" title="Task层面checkpoint的准备工作"></a>Task层面checkpoint的准备工作</h4><p>先说Task类中的部分，该类创建了一个 <code>CheckpointMetaData</code> 的对象，并且生成了一个Runable匿名类用于执行checkpoint，然后以异步的方式触发了该Runable：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public void triggerCheckpointBarrier(</span><br><span class="line">            final long checkpointID,</span><br><span class="line">            long checkpointTimestamp,</span><br><span class="line">            final CheckpointOptions checkpointOptions) &#123;</span><br><span class="line"></span><br><span class="line">            ......</span><br><span class="line"></span><br><span class="line">            Runnable runnable = new Runnable() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void run() &#123;</span><br><span class="line">                    // set safety net from the task&apos;s context for checkpointing thread</span><br><span class="line">                    LOG.debug(&quot;Creating FileSystem stream leak safety net for &#123;&#125;&quot;, Thread.currentThread().getName());</span><br><span class="line">                    FileSystemSafetyNet.setSafetyNetCloseableRegistryForThread(safetyNetCloseableRegistry);</span><br><span class="line"></span><br><span class="line">                    try &#123;</span><br><span class="line">                        boolean success = invokable.triggerCheckpoint(checkpointMetaData, checkpointOptions);</span><br><span class="line">                        if (!success) &#123;</span><br><span class="line">                            checkpointResponder.declineCheckpoint(</span><br><span class="line">                                    getJobID(), getExecutionId(), checkpointID,</span><br><span class="line">                                    new CheckpointDeclineTaskNotReadyException(taskName));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    </span><br><span class="line">                    ......</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">            executeAsyncCallRunnable(runnable, String.format(&quot;Checkpoint Trigger for %s (%s).&quot;, taskNameWithSubtask, executionId));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>上面代码里的invokable事实上就是我们的StreamTask了。Task类实际上是将checkpoint委托给了更具体的类去执行，而StreamTask也将委托给更具体的类，直到业务代码。<br>StreamTask是这样实现的：</p>
<ul>
<li>如果task还在运行，那就可以进行checkpoint。方法是先向下游所有出口广播一个Barrier，然后触发本task的State保存。</li>
<li>如果task结束了，那我们就要通知下游取消本次checkpoint，方法是发送一个CancelCheckpointMarker，这是类似于Barrier的另一种消息。</li>
<li>注意，从这里开始，整个执行链路上开始出现Barrier，可以和前面讲Fault Tolerant原理的地方结合看一下。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">private boolean performCheckpoint(</span><br><span class="line">            CheckpointMetaData checkpointMetaData,</span><br><span class="line">            CheckpointOptions checkpointOptions,</span><br><span class="line">            CheckpointMetrics checkpointMetrics) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        synchronized (lock) &#123;</span><br><span class="line">            if (isRunning) &#123;</span><br><span class="line">            </span><br><span class="line">                operatorChain.broadcastCheckpointBarrier(</span><br><span class="line">                        checkpointMetaData.getCheckpointId(),</span><br><span class="line">                        checkpointMetaData.getTimestamp(),</span><br><span class="line">                        checkpointOptions);</span><br><span class="line"></span><br><span class="line">                checkpointState(checkpointMetaData, checkpointOptions, checkpointMetrics);</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line"></span><br><span class="line">                ......</span><br><span class="line">                </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>完成 <code>broadcastCheckpointBarrier</code> 方法后，在 <code>checkpointState()</code> 方法中，StreamTask还做了很多别的工作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public void executeCheckpointing() throws Exception &#123;</span><br><span class="line">            </span><br><span class="line">            ......</span><br><span class="line"></span><br><span class="line">            try &#123;</span><br><span class="line">                //这里，就是调用StreamOperator进行snapshotState的入口方法</span><br><span class="line">                for (StreamOperator&lt;?&gt; op : allOperators) &#123;</span><br><span class="line">                    checkpointStreamOperator(op);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                // we are transferring ownership over snapshotInProgressList for cleanup to the thread, active on submit</span><br><span class="line">                AsyncCheckpointRunnable asyncCheckpointRunnable = new AsyncCheckpointRunnable(</span><br><span class="line">                    owner,</span><br><span class="line">                    operatorSnapshotsInProgress,</span><br><span class="line">                    checkpointMetaData,</span><br><span class="line">                    checkpointMetrics,</span><br><span class="line">                    startAsyncPartNano);</span><br><span class="line"></span><br><span class="line">                owner.cancelables.registerCloseable(asyncCheckpointRunnable);</span><br><span class="line">                //这里注册了一个Runnable，在执行完checkpoint之后向JobManager发出CompletedCheckPoint消息，这也是fault tolerant两阶段提交的一部分</span><br><span class="line">                owner.asyncOperationsThreadPool.submit(asyncCheckpointRunnable);</span><br><span class="line">                </span><br><span class="line">                ......</span><br><span class="line">            </span><br><span class="line">            &#125; </span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>说到checkpoint，我们印象里最直观的感受肯定是我们的一些做聚合的操作符的状态保存，比如sum的和以及count的值等等。这些内容就是StreamOperator部分将要触发保存的内容。可以看到，除了我们直观的这些操作符的状态保存外，flink的checkpoint做了大量的其他工作。</p>
<p>接下来，我们就把目光转向操作符的checkpoint机制。</p>
<h4 id="操作符的状态保存及barrier传递"><a href="#操作符的状态保存及barrier传递" class="headerlink" title="操作符的状态保存及barrier传递"></a>操作符的状态保存及barrier传递</h4><p>第四章时，我们已经了解了StreamOperator的类关系，这里，我们就直接接着上一节的 <code>checkpointStreamOperator(op)</code> 方法往下讲。<br>顺便，前面也提到了，在进行checkpoint之前，operator初始化时，会执行一个 <code>initializeState</code> 方法，在该方法中，如果task是从失败中恢复的话，其保存的state也会被restore进来。</p>
<p>传递barrier是在进行本operator的statesnapshot之前完成的，我们先来看看其逻辑，其实和传递一条数据是类似的，就是生成一个 <code>CheckpointBarrier</code> 对象，然后向每个streamOutput写进去：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public void broadcastCheckpointBarrier(long id, long timestamp, CheckpointOptions checkpointOptions) throws IOException &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            CheckpointBarrier barrier = new CheckpointBarrier(id, timestamp, checkpointOptions);</span><br><span class="line">            for (RecordWriterOutput&lt;?&gt; streamOutput : streamOutputs) &#123;</span><br><span class="line">                streamOutput.broadcastEvent(barrier);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        catch (InterruptedException e) &#123;</span><br><span class="line">            throw new IOException(&quot;Interrupted while broadcasting checkpoint barrier&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>下游的operator接收到本barrier，就会触发其自身的checkpoint。</p>
<p>StreamTask在执行完broadcastCheckpointBarrier之后，<br>我们当前的wordcount程序里有两个operator chain，分别是：</p>
<ul>
<li>kafka source -&gt; flatmap</li>
<li>keyed aggregation -&gt; sink<br>我们就按这个顺序来捋一下checkpoint的过程。</li>
</ul>
<p>1.kafka source的checkpoint过程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">public final void snapshotState(FunctionSnapshotContext context) throws Exception &#123;</span><br><span class="line">        if (!running) &#123;</span><br><span class="line">            LOG.debug(&quot;snapshotState() called on closed source&quot;);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            unionOffsetStates.clear();</span><br><span class="line"></span><br><span class="line">            final AbstractFetcher&lt;?, ?&gt; fetcher = this.kafkaFetcher;</span><br><span class="line">            if (fetcher == null) &#123;</span><br><span class="line">                // the fetcher has not yet been initialized, which means we need to return the</span><br><span class="line">                // originally restored offsets or the assigned partitions</span><br><span class="line">                for (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">                    unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">                    // the map cannot be asynchronously updated, because only one checkpoint call can happen</span><br><span class="line">                    // on this function at a time: either snapshotState() or notifyCheckpointComplete()</span><br><span class="line">                    pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">                if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">                    // the map cannot be asynchronously updated, because only one checkpoint call can happen</span><br><span class="line">                    // on this function at a time: either snapshotState() or notifyCheckpointComplete()</span><br><span class="line">                    pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                for (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">                    unionOffsetStates.add(</span><br><span class="line">                            Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">                // truncate the map of pending offsets to commit, to prevent infinite growth</span><br><span class="line">                while (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</span><br><span class="line">                    pendingOffsetsToCommit.remove(0);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>kafka的snapshot逻辑就是记录一下当前消费的offsets，然后做成tuple（partitiion，offset）放进一个 <code>StateBackend</code> 里。StateBackend是flink抽象出来的一个用于保存状态的接口。</p>
<p>2.<strong>FlatMap算子的checkpoint过程</strong><br>没什么可说的，就是调用了snapshotState()方法而已。</p>
<p>3.<strong>本operator chain的state保存过程</strong><br>细心的同学应该注意到了，各个算子的snapshot方法只把自己的状态保存到了StateBackend里，没有写入的持久化操作。这部分操作被放到了 <code>AbstractStreamOperator</code> 中，由flink统一负责持久化。其实不需要看源码我们也能想出来，持久化无非就是把这些数据用一个流写到磁盘或者别的地方，接下来我们来看看是不是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//还是AbstractStreamOperator.java的snapshotState方法</span><br><span class="line">            if (null != operatorStateBackend) &#123;</span><br><span class="line">                snapshotInProgress.setOperatorStateManagedFuture(</span><br><span class="line">                    operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>那么这个operatorStateBackend是怎么保存状态的呢？</p>
<ul>
<li>首先把各个算子的state做了一份深拷贝；</li>
<li>然后以异步的方式执行了一个内部类的runnable，该内部类的run方法实现了一个模版方法，首先打开stream，然后写入数据，然后再关闭stream。<br>我们来看看这个写入数据的方法：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public SnapshotResult&lt;OperatorStateHandle&gt; performOperation() throws Exception &#123;</span><br><span class="line">                    long asyncStartTime = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">                    CheckpointStreamFactory.CheckpointStateOutputStream localOut = this.out;</span><br><span class="line"></span><br><span class="line">                    // get the registered operator state infos ...</span><br><span class="line">                    List&lt;RegisteredOperatorBackendStateMetaInfo.Snapshot&lt;?&gt;&gt; operatorMetaInfoSnapshots =</span><br><span class="line">                        new ArrayList&lt;&gt;(registeredOperatorStatesDeepCopies.size());</span><br><span class="line"></span><br><span class="line">                    for (Map.Entry&lt;String, PartitionableListState&lt;?&gt;&gt; entry : registeredOperatorStatesDeepCopies.entrySet()) &#123;</span><br><span class="line">                        operatorMetaInfoSnapshots.add(entry.getValue().getStateMetaInfo().snapshot());</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    // ... write them all in the checkpoint stream ...</span><br><span class="line">                    DataOutputView dov = new DataOutputViewStreamWrapper(localOut);</span><br><span class="line"></span><br><span class="line">                    OperatorBackendSerializationProxy backendSerializationProxy =</span><br><span class="line">                        new OperatorBackendSerializationProxy(operatorMetaInfoSnapshots, broadcastMetaInfoSnapshots);</span><br><span class="line"></span><br><span class="line">                    backendSerializationProxy.write(dov);</span><br><span class="line"></span><br><span class="line">                    ......</span><br><span class="line">                    </span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure>
<p>注释写的很清楚，我就不多说了。</p>
<p>4.<strong>后继operatorChain的checkpoint过程</strong><br>前面说到，在flink的流中，barrier流过时会触发checkpoint。在上面第1步中，上游节点已经发出了Barrier，所以在我们的keyed aggregation -&gt; sink 这个operatorchain中，我们将首先捕获这个barrier。</p>
<p>捕获barrier的过程其实就是处理input数据的过程，对应着 <code>StreamInputProcessor.processInput()</code> 方法，该方法我们在第四章已经讲过，这里我们简单回顾一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//每个元素都会触发这一段逻辑，如果下一个数据是buffer，则从外围的while循环里进入处理用户数据的逻辑；这个方法里默默的处理了barrier的逻辑</span><br><span class="line">            final BufferOrEvent bufferOrEvent = barrierHandler.getNextNonBlocked();</span><br><span class="line">            if (bufferOrEvent != null) &#123;</span><br><span class="line">                if (bufferOrEvent.isBuffer()) &#123;</span><br><span class="line">                    currentChannel = bufferOrEvent.getChannelIndex();</span><br><span class="line">                    currentRecordDeserializer = recordDeserializers[currentChannel];</span><br><span class="line">                    currentRecordDeserializer.setNextBuffer(bufferOrEvent.getBuffer());</span><br><span class="line">                &#125;</span><br><span class="line">                else &#123;</span><br><span class="line">                    // Event received</span><br><span class="line">                    final AbstractEvent event = bufferOrEvent.getEvent();</span><br><span class="line">                    if (event.getClass() != EndOfPartitionEvent.class) &#123;</span><br><span class="line">                        throw new IOException(&quot;Unexpected event: &quot; + event);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>处理barrier的过程在这段代码里没有体现，因为被包含在了 <code>getNextNonBlocked()</code> 方法中，我们看下这个方法的核心逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//BarrierBuffer.getNextNonBlocked方法</span><br><span class="line">            else if (bufferOrEvent.getEvent().getClass() == CheckpointBarrier.class) &#123;</span><br><span class="line">                if (!endOfStream) &#123;</span><br><span class="line">                    // process barriers only if there is a chance of the checkpoint completing</span><br><span class="line">                    processBarrier((CheckpointBarrier) bufferOrEvent.getEvent(), bufferOrEvent.getChannelIndex());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            else if (bufferOrEvent.getEvent().getClass() == CancelCheckpointMarker.class) &#123;</span><br><span class="line">                processCancellationBarrier((CancelCheckpointMarker) bufferOrEvent.getEvent());</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>先提一嘴，大家还记得之前的部分也提到过CheckpointMarker吧，这里正好也对上了。</p>
<p>处理barrier也是个麻烦事，大家回想一下5.1节提到的屏障的原理图，一个opertor必须收到从每个inputchannel发过来的同一序号的barrier之后才能发起本节点的checkpoint，如果有的channel的数据处理的快了，那该barrier后的数据还需要缓存起来，如果有的inputchannel被关闭了，那它就不会再发送barrier过来了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">private void processBarrier(CheckpointBarrier receivedBarrier, int channelIndex) throws Exception &#123;</span><br><span class="line">        final long barrierId = receivedBarrier.getId();</span><br><span class="line"></span><br><span class="line">        // fast path for single channel cases</span><br><span class="line">        if (totalNumberOfInputChannels == 1) &#123;</span><br><span class="line">            if (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">                // new checkpoint</span><br><span class="line">                currentCheckpointId = barrierId;</span><br><span class="line">                notifyCheckpoint(receivedBarrier);</span><br><span class="line">            &#125;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // -- general code path for multiple input channels --</span><br><span class="line"></span><br><span class="line">        if (numBarriersReceived &gt; 0) &#123;</span><br><span class="line">            // this is only true if some alignment is already progress and was not canceled</span><br><span class="line"></span><br><span class="line">            if (barrierId == currentCheckpointId) &#123;</span><br><span class="line">                // regular case</span><br><span class="line">                onBarrier(channelIndex);</span><br><span class="line">            &#125;</span><br><span class="line">            else if (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">                // we did not complete the current checkpoint, another started before</span><br><span class="line">                LOG.warn(&quot;Received checkpoint barrier for checkpoint &#123;&#125; before completing current checkpoint &#123;&#125;. &quot; +</span><br><span class="line">                        &quot;Skipping current checkpoint.&quot;, barrierId, currentCheckpointId);</span><br><span class="line"></span><br><span class="line">                // let the task know we are not completing this</span><br><span class="line">                notifyAbort(currentCheckpointId, new CheckpointDeclineSubsumedException(barrierId));</span><br><span class="line"></span><br><span class="line">                // abort the current checkpoint</span><br><span class="line">                releaseBlocksAndResetBarriers();</span><br><span class="line"></span><br><span class="line">                // begin a the new checkpoint</span><br><span class="line">                beginNewAlignment(barrierId, channelIndex);</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // ignore trailing barrier from an earlier checkpoint (obsolete now)</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        else if (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">            // first barrier of a new checkpoint</span><br><span class="line">            beginNewAlignment(barrierId, channelIndex);</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">            // either the current checkpoint was canceled (numBarriers == 0) or</span><br><span class="line">            // this barrier is from an old subsumed checkpoint</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // check if we have all barriers - since canceled checkpoints always have zero barriers</span><br><span class="line">        // this can only happen on a non canceled checkpoint</span><br><span class="line">        if (numBarriersReceived + numClosedChannels == totalNumberOfInputChannels) &#123;</span><br><span class="line">            // actually trigger checkpoint</span><br><span class="line">            if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">                LOG.debug(&quot;Received all barriers, triggering checkpoint &#123;&#125; at &#123;&#125;&quot;,</span><br><span class="line">                        receivedBarrier.getId(), receivedBarrier.getTimestamp());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            releaseBlocksAndResetBarriers();</span><br><span class="line">            notifyCheckpoint(receivedBarrier);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>总之，当收到全部的barrier之后，就会触发 <code>notifyCheckpoint()</code> ，该方法又会调用StreamTask的 <code>triggerCheckpoint</code> ，和之前的operator是一样的。</p>
<p>如果还有后续的operator的话，就是完全相同的循环，不再赘述。</p>
<p>5.<strong>报告完成checkpoint事件</strong><br>当一个operator保存完checkpoint数据后，就会启动一个异步对象 <code>AsyncCheckpointRunnable</code> ，用以报告该检查点已完成，其具体逻辑在reportCompletedSnapshotStates中。这个方法把任务又最终委托给了 <code>RpcCheckpointResponder</code> 这个类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">checkpointResponder.acknowledgeCheckpoint(</span><br><span class="line">            jobId,</span><br><span class="line">            executionAttemptID,</span><br><span class="line">            checkpointId,</span><br><span class="line">            checkpointMetrics,</span><br><span class="line">            acknowledgedState);</span><br></pre></td></tr></table></figure>
<p>从这个类也可以看出来，它的逻辑是通过rpc的方式远程调JobManager的相关方法完成报告事件，底层也是通过akka实现的。<br>那么，谁响应了这个rpc调用呢？是该任务的JobMaster。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//JobMaster.java</span><br><span class="line">    public void acknowledgeCheckpoint(</span><br><span class="line">            final JobID jobID,</span><br><span class="line">            final ExecutionAttemptID executionAttemptID,</span><br><span class="line">            final long checkpointId,</span><br><span class="line">            final CheckpointMetrics checkpointMetrics,</span><br><span class="line">            final TaskStateSnapshot checkpointState) &#123;</span><br><span class="line"></span><br><span class="line">        final CheckpointCoordinator checkpointCoordinator = executionGraph.getCheckpointCoordinator();</span><br><span class="line">        final AcknowledgeCheckpoint ackMessage = new AcknowledgeCheckpoint(</span><br><span class="line">            jobID,</span><br><span class="line">            executionAttemptID,</span><br><span class="line">            checkpointId,</span><br><span class="line">            checkpointMetrics,</span><br><span class="line">            checkpointState);</span><br><span class="line"></span><br><span class="line">        if (checkpointCoordinator != null) &#123;</span><br><span class="line">            getRpcService().execute(() -&gt; &#123;</span><br><span class="line">                try &#123;</span><br><span class="line">                    checkpointCoordinator.receiveAcknowledgeMessage(ackMessage);</span><br><span class="line">                &#125; catch (Throwable t) &#123;</span><br><span class="line">                    log.warn(&quot;Error while processing checkpoint acknowledgement message&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            log.error(&quot;Received AcknowledgeCheckpoint message for job &#123;&#125; with no CheckpointCoordinator&quot;,</span><br><span class="line">                    jobGraph.getJobID());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>JobMaster反手就是一巴掌就把任务又rpc给了 <code>CheckpointCoordinator.receiveAcknowledgeMessage()</code> 方法。</p>
<p>之前提到，coordinator在触发checkpoint时，生成了一个 <code>PendingCheckpoint</code> ，保存了所有operator的id。</p>
<p>当PendingCheckpoint收到一个operator的完成checkpoint的消息时，它就把这个operator从未完成checkpoint的节点集合移动到已完成的集合。当所有的operator都报告完成了checkpoint时，CheckpointCoordinator会触发 <code>completePendingCheckpoint()</code> 方法，该方法做了以下事情：</p>
<ul>
<li>把pendinCgCheckpoint转换为CompletedCheckpoint</li>
<li>把CompletedCheckpoint加入已完成的检查点集合，并从未完成检查点集合删除该检查点</li>
<li>再度向各个operator发出rpc，通知该检查点已完成<br>本文里，收到这个远程调用的就是那两个operator chain，我们来看看其逻辑:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void notifyCheckpointComplete(long checkpointId) throws Exception &#123;</span><br><span class="line">        synchronized (lock) &#123;</span><br><span class="line">            if (isRunning) &#123;</span><br><span class="line">                LOG.debug(&quot;Notification of complete checkpoint for task &#123;&#125;&quot;, getName());</span><br><span class="line"></span><br><span class="line">                for (StreamOperator&lt;?&gt; operator : operatorChain.getAllOperators()) &#123;</span><br><span class="line">                    if (operator != null) &#123;</span><br><span class="line">                        operator.notifyCheckpointComplete(checkpointId);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                LOG.debug(&quot;Ignoring notification of complete checkpoint for not-running task &#123;&#125;&quot;, getName());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>再接下来无非就是层层通知对应的算子做出响应罢了。<br>至此，flink的两阶段提交的checkpoint逻辑全部完成。</p>
<h3 id="承载checkpoint数据的抽象：State-amp-StateBackend"><a href="#承载checkpoint数据的抽象：State-amp-StateBackend" class="headerlink" title="承载checkpoint数据的抽象：State &amp; StateBackend"></a>承载checkpoint数据的抽象：State &amp; StateBackend</h3><p>State是快照数据的载体，StateBackend是快照如何被保存的抽象。</p>
<p>State分为 KeyedState和OperatorState，从名字就可以看出来分别对应着keyedStream和其他的oeprator。从State由谁管理上，也可以区分为raw state和Managed state。Flink管理的就是Managed state，用户自己管理的就是raw state。Managed State又分为ValueState、ListState、ReducingState、AggregatingState、FoldingState、MapState这么几种，看名字知用途。</p>
<p>StateBackend目前提供了三个backend，MemoryStateBackend，FsStateBackend，RocksDBStateBackend，都是看名字知用途系列。</p>
<p>State接口、StateBackend接口及其实现都比较简单，代码就不贴了， 尤其State本质上就是一层容器封装。</p>
<p>贴个别人写的状态管理的文章吧： <a href="https://yq.aliyun.com/articles/225623?spm=a2c4e.11153940.blogcont225624.12.7c797f6bZo3tiM" target="_blank" rel="noopener">详解Flink中的状态管理</a></p>
<h2 id="数据流转——Flink的数据抽象及数据交换过程"><a href="#数据流转——Flink的数据抽象及数据交换过程" class="headerlink" title="数据流转——Flink的数据抽象及数据交换过程"></a>数据流转——Flink的数据抽象及数据交换过程</h2><p>本章打算讲一下flink底层是如何定义和在操作符之间传递数据的。</p>
<h3 id="flink的数据抽象"><a href="#flink的数据抽象" class="headerlink" title="flink的数据抽象"></a>flink的数据抽象</h3><h4 id="MemorySegment"><a href="#MemorySegment" class="headerlink" title="MemorySegment"></a>MemorySegment</h4><p>Flink作为一个高效的流框架，为了避免JVM的固有缺陷（java对象存储密度低，FGC影响吞吐和响应等），必然走上自主管理内存的道路。</p>
<p>这个 <code>MemorySegment</code> 就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。</p>
<p>如果说byte[]数组和direct memory是最底层的存储，那么memorysegment就是在其上覆盖的一层统一抽象。它定义了一系列抽象方法，用于控制和底层内存的交互，如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public abstract class MemorySegment &#123;</span><br><span class="line"></span><br><span class="line">    public abstract byte get(int index);</span><br><span class="line">    </span><br><span class="line">    public abstract void put(int index, byte b);</span><br><span class="line">    </span><br><span class="line">    public int size() ;</span><br><span class="line">    </span><br><span class="line">    public abstract ByteBuffer wrap(int offset, int length);</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到，它在提供了诸多直接操作内存的方法外，还提供了一个 <code>wrap()</code> 方法，将自己包装成一个ByteBuffer，我们待会儿讲这个ByteBuffer。</p>
<p>Flink为MemorySegment提供了两个实现类： <code>HeapMemorySegment</code> 和 <code>HybridMemorySegment</code> 。他们的区别在于前者只能分配堆内存，而后者能用来分配堆内和堆外内存。事实上，Flink框架里，只使用了后者。这是为什么呢？</p>
<p>如果HybridMemorySegment只能用于分配堆外内存的话，似乎更合常理。但是在JVM的世界中，如果一个方法是一个虚方法，那么每次调用时，JVM都要花时间去确定调用的到底是哪个子类实现的该虚方法（方法重写机制，不明白的去看JVM的invokeVirtual指令），也就意味着每次都要去翻方法表；而如果该方法虽然是个虚方法，但实际上整个JVM里只有一个实现（就是说只加载了一个子类进来），那么JVM会很聪明的把它去虚化处理，这样就不用每次调用方法时去找方法表了，能够大大提升性能。但是只分配堆内或者堆外内存不能满足我们的需要，所以就出现了HybridMemorySegment同时可以分配两种内存的设计。</p>
<p>我们可以看看HybridMemorySegment的构造代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">HybridMemorySegment(ByteBuffer buffer, Object owner) &#123;</span><br><span class="line">        super(checkBufferAndGetAddress(buffer), buffer.capacity(), owner);</span><br><span class="line">        this.offHeapBuffer = buffer;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">        HybridMemorySegment(byte[] buffer, Object owner) &#123;</span><br><span class="line">        super(buffer, owner);</span><br><span class="line">        this.offHeapBuffer = null;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>其中，第一个构造函数的 <code>checkBufferAndGetAddress()</code> 方法能够得到direct buffer的内存地址，因此可以操作堆外内存。</p>
<h4 id="ByteBuffer与NetworkBufferPool"><a href="#ByteBuffer与NetworkBufferPool" class="headerlink" title="ByteBuffer与NetworkBufferPool"></a>ByteBuffer与NetworkBufferPool</h4><p>在 <code>MemorySegment</code> 这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是 <code>Buffer</code> 。</p>
<p><strong>注意</strong> ，这个Buffer是个flink接口，不是java.nio提供的那个Buffer抽象类。Flink在这一层面同时使用了这两个同名概念，用来存储对象，直接看代码时到处都是各种xxxBuffer很容易混淆：</p>
<ul>
<li>java提供的那个Buffer抽象类在这一层主要用于构建 <code>HeapByteBuffer</code> ，这个主要是当数据从jvm里的一个对象被序列化成字节数组时用的；</li>
<li>Flink的这个Buffer接口主要是一种flink层面用于传输数据和事件的统一抽象，其实现类是 <code>NetworkBuffer</code> ，是对 <code>MemorySegment</code> 的包装。Flink在各个TaskManager之间传递数据时，使用的是这一层的抽象。<br>因为Buffer的底层是MemorySegment，这可能不是JVM所管理的，所以为了知道什么时候一个Buffer用完了可以回收，Flink引入了引用计数的概念，当确认这个buffer没有人引用，就可以回收这一片MemorySegment用于别的地方了（JVM的垃圾回收为啥不用引用计数？读者思考一下）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf &#123;</span><br><span class="line"></span><br><span class="line">    private volatile int refCnt = 1;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便管理 <code>NetworkBuffer</code> ，Flink提供了 <code>BufferPoolFactory</code> ，并且提供了唯一实现 <code>NetworkBufferPool</code> ，这是个工厂模式的应用。</p>
<p>NetworkBufferPool在每个TaskManager上只有一个，负责所有子task的内存管理。其实例化时就会尝试获取所有可由它管理的内存（对于堆内存来说，直接获取所有内存并放入老年代，并令用户对象只在新生代存活，可以极大程度的减少Full GC），我们看看其构造方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public NetworkBufferPool(int numberOfSegmentsToAllocate, int segmentSize) &#123;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">        try &#123;</span><br><span class="line">            this.availableMemorySegments = new ArrayBlockingQueue&lt;&gt;(numberOfSegmentsToAllocate);</span><br><span class="line">        &#125;</span><br><span class="line">        catch (OutOfMemoryError err) &#123;</span><br><span class="line">            throw new OutOfMemoryError(&quot;Could not allocate buffer queue of length &quot;</span><br><span class="line">                    + numberOfSegmentsToAllocate + &quot; - &quot; + err.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            for (int i = 0; i &lt; numberOfSegmentsToAllocate; i++) &#123;</span><br><span class="line">                ByteBuffer memory = ByteBuffer.allocateDirect(segmentSize);</span><br><span class="line">                availableMemorySegments.add(MemorySegmentFactory.wrapPooledOffHeapMemory(memory, null));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">        long allocatedMb = (sizeInLong * availableMemorySegments.size()) &gt;&gt; 20;</span><br><span class="line"></span><br><span class="line">        LOG.info(&quot;Allocated &#123;&#125; MB for network buffer pool (number of memory segments: &#123;&#125;, bytes per segment: &#123;&#125;).&quot;,</span><br><span class="line">                allocatedMb, availableMemorySegments.size(), segmentSize);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>由于NetworkBufferPool只是个工厂，实际的内存池是 <code>LocalBufferPool</code> 。每个TaskManager都只有一个NetworkBufferPool工厂，但是上面运行的每个task都要有一个和其他task隔离的LocalBufferPool池，这从逻辑上很好理解。另外，NetworkBufferPool会计算自己所拥有的所有内存分片数，在分配新的内存池时对每个内存池应该占有的内存分片数重分配，步骤是：</p>
<ul>
<li>首先，从整个工厂管理的内存片中拿出所有的内存池所需要的最少Buffer数目总和</li>
<li>如果正好分配完，就结束</li>
<li>其次，把所有的剩下的没分配的内存片，按照每个LocalBufferPool内存池的剩余想要容量大小进行按比例分配</li>
<li>剩余想要容量大小是这么个东西：如果该内存池至少需要3个buffer，最大需要10个buffer，那么它的剩余想要容量就是7<br>实现代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">private void redistributeBuffers() throws IOException &#123;</span><br><span class="line">        assert Thread.holdsLock(factoryLock);</span><br><span class="line"></span><br><span class="line">        // All buffers, which are not among the required ones</span><br><span class="line">        final int numAvailableMemorySegment = totalNumberOfMemorySegments - numTotalRequiredBuffers;</span><br><span class="line"></span><br><span class="line">        if (numAvailableMemorySegment == 0) &#123;</span><br><span class="line">            // in this case, we need to redistribute buffers so that every pool gets its minimum</span><br><span class="line">            for (LocalBufferPool bufferPool : allBufferPools) &#123;</span><br><span class="line">                bufferPool.setNumBuffers(bufferPool.getNumberOfRequiredMemorySegments());</span><br><span class="line">            &#125;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        long totalCapacity = 0; // long to avoid int overflow</span><br><span class="line"></span><br><span class="line">        for (LocalBufferPool bufferPool : allBufferPools) &#123;</span><br><span class="line">            int excessMax = bufferPool.getMaxNumberOfMemorySegments() -</span><br><span class="line">                bufferPool.getNumberOfRequiredMemorySegments();</span><br><span class="line">            totalCapacity += Math.min(numAvailableMemorySegment, excessMax);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // no capacity to receive additional buffers?</span><br><span class="line">        if (totalCapacity == 0) &#123;</span><br><span class="line">            return; // necessary to avoid div by zero when nothing to re-distribute</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        final int memorySegmentsToDistribute = MathUtils.checkedDownCast(</span><br><span class="line">                Math.min(numAvailableMemorySegment, totalCapacity));</span><br><span class="line"></span><br><span class="line">        long totalPartsUsed = 0; // of totalCapacity</span><br><span class="line">        int numDistributedMemorySegment = 0;</span><br><span class="line">        for (LocalBufferPool bufferPool : allBufferPools) &#123;</span><br><span class="line">            int excessMax = bufferPool.getMaxNumberOfMemorySegments() -</span><br><span class="line">                bufferPool.getNumberOfRequiredMemorySegments();</span><br><span class="line"></span><br><span class="line">            // shortcut</span><br><span class="line">            if (excessMax == 0) &#123;</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            totalPartsUsed += Math.min(numAvailableMemorySegment, excessMax);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            final int mySize = MathUtils.checkedDownCast(</span><br><span class="line">                    memorySegmentsToDistribute * totalPartsUsed / totalCapacity - numDistributedMemorySegment);</span><br><span class="line"></span><br><span class="line">            numDistributedMemorySegment += mySize;</span><br><span class="line">            bufferPool.setNumBuffers(bufferPool.getNumberOfRequiredMemorySegments() + mySize);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        assert (totalPartsUsed == totalCapacity);</span><br><span class="line">        assert (numDistributedMemorySegment == memorySegmentsToDistribute);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>接下来说说这个 <code>LocalBufferPool</code> 内存池。<br>LocalBufferPool的逻辑想想无非是增删改查，值得说的是其fields：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/** 该内存池需要的最少内存片数目*/</span><br><span class="line">    private final int numberOfRequiredMemorySegments;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 当前已经获得的内存片中，还没有写入数据的空白内存片</span><br><span class="line">     */</span><br><span class="line">    private final ArrayDeque&lt;MemorySegment&gt; availableMemorySegments = new ArrayDeque&lt;MemorySegment&gt;();</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 注册的所有监控buffer可用性的监听器</span><br><span class="line">     */</span><br><span class="line">    private final ArrayDeque&lt;BufferListener&gt; registeredListeners = new ArrayDeque&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    /** 能给内存池分配的最大分片数*/</span><br><span class="line">    private final int maxNumberOfMemorySegments;</span><br><span class="line"></span><br><span class="line">    /** 当前内存池大小 */</span><br><span class="line">    private int currentPoolSize;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 所有经由NetworkBufferPool分配的，被本内存池引用到的（非直接获得的）分片数</span><br><span class="line">     */</span><br><span class="line">    private int numberOfRequestedMemorySegments;</span><br></pre></td></tr></table></figure>
<p>承接NetworkBufferPool的重分配方法，我们来看看LocalBufferPool的 <code>setNumBuffers()</code> 方法，代码很短，逻辑也相当简单，就不展开说了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public void setNumBuffers(int numBuffers) throws IOException &#123;</span><br><span class="line">        synchronized (availableMemorySegments) &#123;</span><br><span class="line">            checkArgument(numBuffers &gt;= numberOfRequiredMemorySegments,</span><br><span class="line">                    &quot;Buffer pool needs at least %s buffers, but tried to set to %s&quot;,</span><br><span class="line">                    numberOfRequiredMemorySegments, numBuffers);</span><br><span class="line"></span><br><span class="line">            if (numBuffers &gt; maxNumberOfMemorySegments) &#123;</span><br><span class="line">                currentPoolSize = maxNumberOfMemorySegments;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                currentPoolSize = numBuffers;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            returnExcessMemorySegments();</span><br><span class="line"></span><br><span class="line">            // If there is a registered owner and we have still requested more buffers than our</span><br><span class="line">            // size, trigger a recycle via the owner.</span><br><span class="line">            if (owner != null &amp;&amp; numberOfRequestedMemorySegments &gt; currentPoolSize) &#123;</span><br><span class="line">                owner.releaseMemory(numberOfRequestedMemorySegments - numBuffers);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="RecordWriter与Record"><a href="#RecordWriter与Record" class="headerlink" title="RecordWriter与Record"></a>RecordWriter与Record</h4><p>我们接着往高层抽象走，刚刚提到了最底层内存抽象是MemorySegment，用于数据传输的是Buffer，那么，承上启下对接从Java对象转为Buffer的中间对象是什么呢？是 <code>StreamRecord</code> 。</p>
<p>从 <code>StreamRecord&lt;T&gt;</code> 这个类名字就可以看出来，这个类就是个wrap，里面保存了原始的Java对象。另外，StreamRecord还保存了一个timestamp。</p>
<p>那么这个对象是怎么变成LocalBufferPool内存池里的一个大号字节数组的呢？借助了 <code>StreamWriter</code> 这个类。</p>
<p>我们直接来看把数据序列化交出去的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private void sendToTarget(T record, int targetChannel) throws IOException, InterruptedException &#123;</span><br><span class="line">        RecordSerializer&lt;T&gt; serializer = serializers[targetChannel];</span><br><span class="line"></span><br><span class="line">        SerializationResult result = serializer.addRecord(record);</span><br><span class="line"></span><br><span class="line">        while (result.isFullBuffer()) &#123;</span><br><span class="line">            if (tryFinishCurrentBufferBuilder(targetChannel, serializer)) &#123;</span><br><span class="line">                // If this was a full record, we are done. Not breaking</span><br><span class="line">                // out of the loop at this point will lead to another</span><br><span class="line">                // buffer request before breaking out (that would not be</span><br><span class="line">                // a problem per se, but it can lead to stalls in the</span><br><span class="line">                // pipeline).</span><br><span class="line">                if (result.isFullRecord()) &#123;</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            BufferBuilder bufferBuilder = requestNewBufferBuilder(targetChannel);</span><br><span class="line"></span><br><span class="line">            result = serializer.continueWritingWithNextBufferBuilder(bufferBuilder);</span><br><span class="line">        &#125;</span><br><span class="line">        checkState(!serializer.hasSerializedData(), &quot;All data should be written at once&quot;);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        if (flushAlways) &#123;</span><br><span class="line">            targetPartition.flush(targetChannel);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>先说最后一行，如果配置为flushAlways，那么会立刻把元素发送出去，但是这样吞吐量会下降；Flink的默认设置其实也不是一个元素一个元素的发送，是单独起了一个线程，每隔固定时间flush一次所有channel，较真起来也算是mini batch了。</p>
<p>再说序列化那一句:<code>SerializationResult result = serializer.addRecord(record);</code> 。在这行代码中，Flink把对象调用该对象所属的序列化器序列化为字节数组。</p>
<h3 id="数据流转过程"><a href="#数据流转过程" class="headerlink" title="数据流转过程"></a>数据流转过程</h3><p>上一节讲了各层数据的抽象，这一节讲讲数据在各个task之间exchange的过程。</p>
<h4 id="整体过程"><a href="#整体过程" class="headerlink" title="整体过程"></a>整体过程</h4><p>看这张图：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cetavukjja42ce1261v5k57i9.png" alt=""></p>
<ol>
<li>第一步必然是准备一个ResultPartition；</li>
<li>通知JobMaster；</li>
<li>JobMaster通知下游节点；如果下游节点尚未部署，则部署之；</li>
<li>下游节点向上游请求数据</li>
<li>开始传输数据</li>
</ol>
<h4 id="数据跨task传递"><a href="#数据跨task传递" class="headerlink" title="数据跨task传递"></a>数据跨task传递</h4><p>本节讲一下算子之间具体的数据传输过程。也先上一张图：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfmpba9v15anggtvsba2o1277m.png" alt=""></p>
<p>数据在task之间传递有如下几步：</p>
<ol>
<li>数据在本operator处理完后，交给 <code>RecordWriter</code> 。每条记录都要选择一个下游节点，所以要经过 <code>ChannelSelector</code> 。</li>
<li>每个channel都有一个serializer（我认为这应该是为了避免多线程写的麻烦），把这条Record序列化为ByteBuffer</li>
<li>接下来数据被写入ResultPartition下的各个subPartition里，此时该数据已经存入DirectBuffer（MemorySegment）</li>
<li>单独的线程控制数据的flush速度，一旦触发flush，则通过Netty的nio通道向对端写入</li>
<li>对端的netty client接收到数据，decode出来，把数据拷贝到buffer里，然后通知 <code>InputChannel</code></li>
<li>有可用的数据时，下游算子从阻塞醒来，从InputChannel取出buffer，再解序列化成record，交给算子执行用户代码<br>数据在不同机器的算子之间传递的步骤就是以上这些。</li>
</ol>
<p>了解了步骤之后，再来看一下部分关键代码：<br>首先是把数据交给recordwriter。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//RecordWriterOutput.java</span><br><span class="line">    @Override</span><br><span class="line">    public void collect(StreamRecord&lt;OUT&gt; record) &#123;</span><br><span class="line">        if (this.outputTag != null) &#123;</span><br><span class="line">            // we are only responsible for emitting to the main input</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        //这里可以看到把记录交给了recordwriter</span><br><span class="line">        pushToRecordWriter(record);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>然后recordwriter把数据发送到对应的通道。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">//RecordWriter.java</span><br><span class="line">    public void emit(T record) throws IOException, InterruptedException &#123;</span><br><span class="line">        //channelselector登场了</span><br><span class="line">        for (int targetChannel : channelSelector.selectChannels(record, numChannels)) &#123;</span><br><span class="line">            sendToTarget(record, targetChannel);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">        private void sendToTarget(T record, int targetChannel) throws IOException, InterruptedException &#123;</span><br><span class="line">        </span><br><span class="line">        //选择序列化器并序列化数据</span><br><span class="line">        RecordSerializer&lt;T&gt; serializer = serializers[targetChannel];</span><br><span class="line"></span><br><span class="line">        SerializationResult result = serializer.addRecord(record);</span><br><span class="line"></span><br><span class="line">        while (result.isFullBuffer()) &#123;</span><br><span class="line">            if (tryFinishCurrentBufferBuilder(targetChannel, serializer)) &#123;</span><br><span class="line">                // If this was a full record, we are done. Not breaking</span><br><span class="line">                // out of the loop at this point will lead to another</span><br><span class="line">                // buffer request before breaking out (that would not be</span><br><span class="line">                // a problem per se, but it can lead to stalls in the</span><br><span class="line">                // pipeline).</span><br><span class="line">                if (result.isFullRecord()) &#123;</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            BufferBuilder bufferBuilder = requestNewBufferBuilder(targetChannel);</span><br><span class="line"></span><br><span class="line">            //写入channel</span><br><span class="line">            result = serializer.continueWritingWithNextBufferBuilder(bufferBuilder);</span><br><span class="line">        &#125;</span><br><span class="line">        checkState(!serializer.hasSerializedData(), &quot;All data should be written at once&quot;);</span><br><span class="line"></span><br><span class="line">        if (flushAlways) &#123;</span><br><span class="line">            targetPartition.flush(targetChannel);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>接下来是把数据推给底层设施（netty）的过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//ResultPartition.java</span><br><span class="line">    @Override</span><br><span class="line">    public void flushAll() &#123;</span><br><span class="line">        for (ResultSubpartition subpartition : subpartitions) &#123;</span><br><span class="line">            subpartition.flush();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">        //PartitionRequestQueue.java</span><br><span class="line">        void notifyReaderNonEmpty(final NetworkSequenceViewReader reader) &#123;</span><br><span class="line">        //这里交给了netty server线程去推</span><br><span class="line">        ctx.executor().execute(new Runnable() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                ctx.pipeline().fireUserEventTriggered(reader);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>netty相关的部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">//AbstractChannelHandlerContext.java</span><br><span class="line">    public ChannelHandlerContext fireUserEventTriggered(final Object event) &#123;</span><br><span class="line">        if (event == null) &#123;</span><br><span class="line">            throw new NullPointerException(&quot;event&quot;);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            final AbstractChannelHandlerContext next = this.findContextInbound();</span><br><span class="line">            EventExecutor executor = next.executor();</span><br><span class="line">            if (executor.inEventLoop()) &#123;</span><br><span class="line">                next.invokeUserEventTriggered(event);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                executor.execute(new OneTimeTask() &#123;</span><br><span class="line">                    public void run() &#123;</span><br><span class="line">                        next.invokeUserEventTriggered(event);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            return this;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>最后真实的写入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">//PartittionRequesetQueue.java</span><br><span class="line">    private void enqueueAvailableReader(final NetworkSequenceViewReader reader) throws Exception &#123;</span><br><span class="line">        if (reader.isRegisteredAsAvailable() || !reader.isAvailable()) &#123;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        // Queue an available reader for consumption. If the queue is empty,</span><br><span class="line">        // we try trigger the actual write. Otherwise this will be handled by</span><br><span class="line">        // the writeAndFlushNextMessageIfPossible calls.</span><br><span class="line">        boolean triggerWrite = availableReaders.isEmpty();</span><br><span class="line">        registerAvailableReader(reader);</span><br><span class="line"></span><br><span class="line">        if (triggerWrite) &#123;</span><br><span class="line">            writeAndFlushNextMessageIfPossible(ctx.channel());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    private void writeAndFlushNextMessageIfPossible(final Channel channel) throws IOException &#123;</span><br><span class="line">        </span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">                next = reader.getNextBuffer();</span><br><span class="line">                if (next == null) &#123;</span><br><span class="line">                    if (!reader.isReleased()) &#123;</span><br><span class="line">                        continue;</span><br><span class="line">                    &#125;</span><br><span class="line">                    markAsReleased(reader.getReceiverId());</span><br><span class="line"></span><br><span class="line">                    Throwable cause = reader.getFailureCause();</span><br><span class="line">                    if (cause != null) &#123;</span><br><span class="line">                        ErrorResponse msg = new ErrorResponse(</span><br><span class="line">                            new ProducerFailedException(cause),</span><br><span class="line">                            reader.getReceiverId());</span><br><span class="line"></span><br><span class="line">                        ctx.writeAndFlush(msg);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    // This channel was now removed from the available reader queue.</span><br><span class="line">                    // We re-add it into the queue if it is still available</span><br><span class="line">                    if (next.moreAvailable()) &#123;</span><br><span class="line">                        registerAvailableReader(reader);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    BufferResponse msg = new BufferResponse(</span><br><span class="line">                        next.buffer(),</span><br><span class="line">                        reader.getSequenceNumber(),</span><br><span class="line">                        reader.getReceiverId(),</span><br><span class="line">                        next.buffersInBacklog());</span><br><span class="line"></span><br><span class="line">                    if (isEndOfPartitionEvent(next.buffer())) &#123;</span><br><span class="line">                        reader.notifySubpartitionConsumed();</span><br><span class="line">                        reader.releaseAllResources();</span><br><span class="line"></span><br><span class="line">                        markAsReleased(reader.getReceiverId());</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    // Write and flush and wait until this is done before</span><br><span class="line">                    // trying to continue with the next buffer.</span><br><span class="line">                    channel.writeAndFlush(msg).addListener(writeListener);</span><br><span class="line"></span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">        </span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>上面这段代码里第二个方法中调用的 <code>writeAndFlush(msg)</code> 就是真正往netty的nio通道里写入的地方了。在这里，写入的是一个RemoteInputChannel，对应的就是下游节点的InputGate的channels。</p>
<p>有写就有读，nio通道的另一端需要读入buffer，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">//CreditBasedPartitionRequestClientHandler.java</span><br><span class="line">    private void decodeMsg(Object msg) throws Throwable &#123;</span><br><span class="line">        final Class&lt;?&gt; msgClazz = msg.getClass();</span><br><span class="line"></span><br><span class="line">        // ---- Buffer --------------------------------------------------------</span><br><span class="line">        if (msgClazz == NettyMessage.BufferResponse.class) &#123;</span><br><span class="line">            NettyMessage.BufferResponse bufferOrEvent = (NettyMessage.BufferResponse) msg;</span><br><span class="line"></span><br><span class="line">            RemoteInputChannel inputChannel = inputChannels.get(bufferOrEvent.receiverId);</span><br><span class="line">            if (inputChannel == null) &#123;</span><br><span class="line">                bufferOrEvent.releaseBuffer();</span><br><span class="line"></span><br><span class="line">                cancelRequestFor(bufferOrEvent.receiverId);</span><br><span class="line"></span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            decodeBufferOrEvent(inputChannel, bufferOrEvent);</span><br><span class="line"></span><br><span class="line">        &#125; </span><br><span class="line">        </span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>插一句，Flink其实做阻塞和获取数据的方式非常自然，利用了生产者和消费者模型，当获取不到数据时，消费者自然阻塞；当数据被加入队列，消费者被notify。Flink的背压机制也是借此实现。</p>
<p>然后在这里又反序列化成 <code>StreamRecord</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//StreamElementSerializer.java</span><br><span class="line">    public StreamElement deserialize(DataInputView source) throws IOException &#123;</span><br><span class="line">        int tag = source.readByte();</span><br><span class="line">        if (tag == TAG_REC_WITH_TIMESTAMP) &#123;</span><br><span class="line">            long timestamp = source.readLong();</span><br><span class="line">            return new StreamRecord&lt;T&gt;(typeSerializer.deserialize(source), timestamp);</span><br><span class="line">        &#125;</span><br><span class="line">        else if (tag == TAG_REC_WITHOUT_TIMESTAMP) &#123;</span><br><span class="line">            return new StreamRecord&lt;T&gt;(typeSerializer.deserialize(source));</span><br><span class="line">        &#125;</span><br><span class="line">        else if (tag == TAG_WATERMARK) &#123;</span><br><span class="line">            return new Watermark(source.readLong());</span><br><span class="line">        &#125;</span><br><span class="line">        else if (tag == TAG_STREAM_STATUS) &#123;</span><br><span class="line">            return new StreamStatus(source.readInt());</span><br><span class="line">        &#125;</span><br><span class="line">        else if (tag == TAG_LATENCY_MARKER) &#123;</span><br><span class="line">            return new LatencyMarker(source.readLong(), new OperatorID(source.readLong(), source.readLong()), source.readInt());</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">            throw new IOException(&quot;Corrupt stream, found tag: &quot; + tag);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>然后再次在 <code>StreamInputProcessor.processInput()</code> 循环中得到处理。</p>
<p>至此，数据在跨jvm的节点之间的流转过程就讲完了。</p>
<h3 id="Credit漫谈"><a href="#Credit漫谈" class="headerlink" title="Credit漫谈"></a>Credit漫谈</h3><p>在看上一部分的代码时，有一个小细节不知道读者有没有注意到，我们的数据发送端的代码叫做 <code>PartittionRequesetQueue.java</code> ，而我们的接收端却起了一个完全不相干的名字： <code>CreditBasedPartitionRequestClientHandler.java</code> 。为什么前面加了CreditBased的前缀呢？</p>
<h4 id="背压问题"><a href="#背压问题" class="headerlink" title="背压问题"></a>背压问题</h4><p>在流模型中，我们期待数据是像水流一样平滑的流过我们的引擎，但现实生活不会这么美好。数据的上游可能因为各种原因数据量暴增，远远超出了下游的瞬时处理能力（回忆一下98年大洪水），导致系统崩溃。<br>那么框架应该怎么应对呢？和人类处理自然灾害的方式类似，我们修建了三峡大坝，当洪水来临时把大量的水囤积在大坝里；对于Flink来说，就是在数据的接收端和发送端放置了缓存池，用以缓冲数据，并且设置闸门阻止数据向下流。</p>
<p>那么Flink又是如何处理背压的呢？答案也是靠这些缓冲池。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfksrl5cd4m1lbqqqgvc811349.png" alt=""></p>
<p>这张图说明了Flink在生产和消费数据时的大致情况。 <code>ResultPartition</code> 和 <code>InputGate</code> 在输出和输入数据时，都要向 <code>NetworkBufferPool</code> 申请一块 <code>MemorySegment</code> 作为缓存池。<br>接下来的情况和生产者消费者很类似。当数据发送太多，下游处理不过来了，那么首先InputChannel会被填满，然后是InputChannel能申请到的内存达到最大，于是下游停止读取数据，上游负责发送数据的nettyServer会得到响应，停止从ResultSubPartition读取缓存，那么ResultPartition很快也将存满数据不能被消费，从而生产数据的逻辑被阻塞在获取新buffer上，非常自然地形成背压的效果。</p>
<p>Flink自己做了个试验用以说明这个机制的效果：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfkta54rkdd1od4aau1e3n7nhm.png" alt=""></p>
<p>我们首先设置生产者的发送速度为60%，然后下游的算子以同样的速度处理数据。然后我们将下游算子的处理速度降低到30%，可以看到上游的生产者的数据产生曲线几乎与消费者同步下滑。而后当我们解除限速，整个流的速度立刻提高到了100%。</p>
<h4 id="使用Credit实现ATM网络流控"><a href="#使用Credit实现ATM网络流控" class="headerlink" title="使用Credit实现ATM网络流控"></a>使用Credit实现ATM网络流控</h4><p>上文已经提到，对于流量控制，一个朴素的思路就是在长江上建三峡链路上建立一个拦截的dam，如下图所示：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfku114lf7hpqf3lmcl0116c13.png" alt=""></p>
<p>基于Credit的流控就是这样一种建立在信用（消费数据的能力)上的，面向每个虚链路（而非端到端的）流模型，如下图所示：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfku4g4g174d7gb5ecbfcib71g.png" alt=""></p>
<p>首先，下游会向上游发送一条credit message，用以通知其目前的信用（可联想信用卡的可用额度），然后上游会根据这个信用消息来决定向下游发送多少数据。当上游把数据发送给下游时，它就从下游的信用卡上划走相应的额度（credit balance）：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfkug5sm1v4l15pbgj4jntc7q1t.png" alt=""></p>
<p>下游总共获得的credit数目是Buf_Alloc，已经消费的数据是Fwd_Cnt，上游发送出来的数据是Tx_Cnt，那么剩下的那部分就是Crd_Bal:<br>Crd_Bal = Buf_Alloc - ( Tx_Cnt - Fwd_Cnt )<br>上面这个式子应该很好理解。</p>
<p>可以看到，Credit Based Flow Control的关键是buffer分配。这种分配可以在数据的发送端完成，也可以在接收端完成。对于下游可能有多个上游节点的情况（比如Flink），使用接收端的credit分配更加合理：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfkvpmlh1gl31ef41cvh1c903a19.png" alt=""></p>
<p>上图中，接收者可以观察到每个上游连接的带宽情况，而上游的节点Snd1却不可能轻易知道发往同一个下游节点的其他Snd2的带宽情况，从而如果在上游控制流量将会很困难，而在下游控制流量将会很方便。</p>
<p>因此，这就是为何Flink在接收端有一个基于Credit的Client，而不是在发送端有一个CreditServer的原因。</p>
<p>最后，再讲一下Credit的面向虚链路的流设计和端到端的流设计的区别：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfl05d2f1ub879c1lc5qsq14n9m.png" alt=""></p>
<p>如上图所示，a是面向连接的流设计，b是端到端的流设计。其中，a的设计使得当下游节点3因某些情况必须缓存数据暂缓处理时，每个上游节点（1和2）都可以利用其缓存保存数据；而端到端的设计b里，只有节点3的缓存才可以用于保存数据（读者可以从如何实现上想想为什么）。</p>
<p>对流控制感兴趣的读者，可以看这篇文章： <a href="https://www.nap.edu/read/5769/chapter/1" target="_blank" rel="noopener">Traffic Management For High-Speed Networks</a> 。</p>
<h2 id="其他核心概念"><a href="#其他核心概念" class="headerlink" title="其他核心概念"></a>其他核心概念</h2><p>截至第六章，和执行过程相关的部分就全部讲完，告一段落了。第七章主要讲一点杂七杂八的内容，有时间就不定期更新。</p>
<h3 id="EventTime时间模型"><a href="#EventTime时间模型" class="headerlink" title="EventTime时间模型"></a>EventTime时间模型</h3><p>flink有三种时间模型：ProcessingTime，EventTime和IngestionTime。<br>关于时间模型看这张图：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cdbotdcmoe11q961st5lbn1j4n9.png" alt=""></p>
<p>从这张图里可以很清楚的看到三种Time模型的区别。</p>
<ul>
<li>EventTime是数据被生产出来的时间，可以是比如传感器发出信号的时间等（此时数据还没有被传输给flink）。</li>
<li>IngestionTime是数据进入flink的时间，也就是从Source进入flink流的时间（此时数据刚刚被传给flink）</li>
<li>ProcessingTime是针对当前算子的系统时间，是指该数据已经进入某个operator时，operator所在系统的当前时间<br>例如，我在写这段话的时间是2018年5月13日03点47分，但是我引用的这张EventTime的图片，是2015年画出来的，那么这张图的EventTime是2015年，而ProcessingTime是现在。<br>Flink官网对于时间戳的解释非常详细： <a href="https://ci.apache.org/projects/flink/flink-docs-master/dev/event_time.html" target="_blank" rel="noopener">点我</a><br>Flink对于EventTime模型的实现，依赖的是一种叫做 <code>watermark</code> 的对象。watermark是携带有时间戳的一个对象，会按照程序的要求被插入到数据流中，用以标志某个事件在该时间发生了。<br>我再做一点简短的说明，还是以官网的图为例：</li>
</ul>
<p><img src="/2018/09/18/flink-code-section-1/image_1cdbt8v5jl2ujn91uu1joh1p4gm.png" alt=""></p>
<p>对于有序到来的数据，假设我们在timestamp为11的元素后加入一个watermark，时间记录为11，则下个元素收到该watermark时，认为所有早于11的元素均已到达。这是非常理想的情况。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cdbtcc5c1a6i1tuaadb1rd5136913.png" alt=""></p>
<p>而在现实生活中，经常会遇到乱序的数据。这时，我们虽然在timestamp为7的元素后就收到了11，但是我们一直等到了收到元素12之后，才插入了watermark为11的元素。与上面的图相比，如果我们仍然在11后就插入11的watermark，那么元素9就会被丢弃，造成数据丢失。而我们在12之后插入watermark11，就保证了9仍然会被下一个operator处理。当然，我们不可能无限制的永远等待迟到元素，所以要在哪个元素后插入11需要根据实际场景权衡。</p>
<p>对于来自多个数据源的watermark，可以看这张图：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cdbufp4a1opmsit5n61mial4520.png" alt=""></p>
<p>可以看到，当一个operator收到多个watermark时，它遵循最小原则（或者说最早），即算子的当前watermark是流经该算子的最小watermark，以容许来自不同的source的乱序数据到来。<br>关于事件时间模型，更多内容可以参考 <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101" target="_blank" rel="noopener">Stream 101</a> 和谷歌的这篇论文： <a href="https://research.google.com/pubs/archive/43864.pdf" target="_blank" rel="noopener">Dataflow Model paper</a></p>
<h3 id="FLIP-6-部署及处理模型演进"><a href="#FLIP-6-部署及处理模型演进" class="headerlink" title="FLIP-6 部署及处理模型演进"></a>FLIP-6 部署及处理模型演进</h3><p>就在老白写这篇blog的时候，Flink发布了其1.5 RELEASE版本，号称实现了其部署及处理模型（也就是FLIP-6)，所以打算简略地说一下FLIP-6的主要内容。</p>
<h4 id="现有模型不足"><a href="#现有模型不足" class="headerlink" title="现有模型不足"></a>现有模型不足</h4><p>1.5之前的Flink模型有很多不足，包括：</p>
<ul>
<li>只能静态分配计算资源</li>
<li>在YARN上所有的资源分配都是一碗水端平的</li>
<li>与Docker/k8s的集成非常之蠢，颇有脱裤子放屁的神韵</li>
<li>JobManager没有任务调度逻辑</li>
<li>任务在YARN上执行结束后web dashboard就不可用</li>
<li>集群的session模式和per job模式混淆难以理解<br>就我个人而言，我觉得Flink有一个这里完全没提到的不足才是最应该修改的：针对任务的完全的资源隔离。尤其是如果用Standalone集群，一个用户的task跑挂了TaskManager，然后拖垮了整个集群的情况简直不要太多。</li>
</ul>
<h4 id="核心变更"><a href="#核心变更" class="headerlink" title="核心变更"></a>核心变更</h4><p><strong>Single Job JobManager</strong><br>最重要的变更是一个JobManager只处理一个job。当我们生成JobGraph时就顺便起一个JobManager，这显然更加自然。</p>
<p><strong>ResourceManager</strong><br>其职责包括获取新的TM和slot，通知失败，释放资源以及缓存TM以用于重用等。重要的是，这个组件要能做到挂掉时不要搞垮正在运行的好好的任务。其职责和与JobManager、TaskManager的交互图如下：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfl9453k1gld4acr1m13j3195sg.png" alt=""><br><strong>TaskManager</strong><br>TM要与上面的两个组件交互。与JobManager交互时，要能提供slot，要能与所有给出slot的JM交互。丢失与JM的连接时要能试图把本TM上的slot的情况通告给新JM，如果这一步失败，就要能重新分配slot。<br>与ResourceManager交互时，要通知RM自己的资源和当前的Job分配情况，能按照RM的要求分配资源或者关闭自身。</p>
<p><strong>JobManager Slot Pool</strong><br>这个pool要持有所有分配给当前job的slot资源，并且能在RM挂掉的情况下管理当前已经持有的slot。</p>
<p><strong>Dispatcher</strong><br>需要一个Job的分发器的主要原因是在有的集群环境下我们可能需要一个统一的提交和监控点，以及替代之前的Standalone模式下的JobManager。将来对分发器的期望可能包括权限控制等。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfl9ju2617bh1s191mar1jsp12vot.png" alt=""></p>
<h4 id="Cluster-Manager的架构"><a href="#Cluster-Manager的架构" class="headerlink" title="Cluster Manager的架构"></a>Cluster Manager的架构</h4><p><strong>YARN</strong><br>新的基于YARN的架构主要包括不再需要先在容器里启动集群，然后提交任务；用户代码不再使用动态ClassLoader加载；不用的资源可以释放；可以按需分配不同大小的容器等。其执行过程如下：<br>无Dispatcher时</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfla0n7u1lg21n3o36uu0c1o5h1a.png" alt=""></p>
<p>有Dispatcher时</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfla15os15i3qcsu6c4p4clk1n.png" alt=""><br><strong>Mesos</strong><br>与基于YARN的模式很像，但是只有带Dispatcher模式，因为只有这样才能在Mesos集群里跑其RM。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfla4tka101n18bf1mno4npu9s24.png" alt=""></p>
<p>Mesos的Fault Tolerance是类似这样的：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cfla6eka1ph71mu1pll1q0mgqq2h.png" alt=""></p>
<p>必须用类似Marathon之类的技术保证Dispatcher的HA。</p>
<p><strong>Standalone</strong><br>其实没啥可说的，把以前的JobManager的职责换成现在的Dispatcher就行了。</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cflaaim2ih2v54umsmq01lqc2u.png" alt=""></p>
<p>将来可能会实现一个类似于轻量级Yarn的模式。</p>
<p><strong>Docker/k8s</strong><br>用户定义好容器，至少有一个是job specific的（不然怎么启动任务）；还有用于启动TM的，可以不是job specific的。启动过程如下</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cflafs2o1trgicjmdbndn1bdq3b.png" alt=""></p>
<h4 id="组件设计及细节"><a href="#组件设计及细节" class="headerlink" title="组件设计及细节"></a>组件设计及细节</h4><p><strong>分配slot相关细节</strong><br>从新的TM取slot过程：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cflakoadvjm8pf6nt1k331qj33o.png" alt=""><br>从Cached TM取slot过程：</p>
<p><img src="/2018/09/18/flink-code-section-1/image_1cflambu91ufi5fl1cg9gimdff45.png" alt=""><br><strong>失败处理</strong></p>
<ol>
<li><p>TM失败<br>TM失败时，RM要能检测到失败，更新自己的状态，发送消息给JM，重启一份TM；JM要能检测到失败，从状态移除失效slot，标记该TM的task为失败，并在没有足够slot继续任务时调整规模；TM自身则要能从Checkpoint恢复</p>
</li>
<li><p>RM失败<br>此时TM要能检测到失败，并准备向新的RM注册自身，并且向新的RM传递自身的资源情况；JM要能检测到失败并且等待新的RM可用，重新请求需要的资源；丢失的数据要能从Container、TM等处恢复。</p>
</li>
<li><p>JM失败<br>TM释放所有task，向新JM注册资源，并且如果不成功，就向RM报告这些资源可用于重分配；RM坐等；JM丢失的数据从持久化存储中获得，已完成的checkpoints从HA恢复，从最近的checkpoint重启task，并申请资源。</p>
</li>
<li><p>JM &amp; RM 失败<br>TM将在一段时间内试图把资源交给新上任的JM，如果失败，则把资源交给新的RM</p>
</li>
<li><p>TM &amp; RM失败<br>JM如果正在申请资源，则要等到新的RM启动后才能获得；JM可能需要调整其规模，因为损失了TM的slot。</p>
</li>
</ol>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>Flink是当前流处理领域的优秀框架，其设计思想和代码实现都蕴含着许多人的智慧结晶。这篇解读花了很多时间，篇幅也写了很长，也仍然不能能覆盖Flink的方方面面，也肯定有很多错误之处，欢迎大家批评指正！</p>
<p>本文转自：老白讲互联网</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/flink/" rel="tag"># flink</a>
          
            <a href="/tags/流式计算/" rel="tag"># 流式计算</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/15/KCon2018议题PPT公开/" rel="next" title="KCon 2018 议题 PPT 公开">
                <i class="fa fa-chevron-left"></i> KCon 2018 议题 PPT 公开
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/20/what-is-pac/" rel="prev" title="什么是PAC">
                什么是PAC <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="xavier ma" />
            
              <p class="site-author-name" itemprop="name">xavier ma</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/maxavier-git" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:maxavier@126.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从-Hello-World-WordCount开始"><span class="nav-number">2.</span> <span class="nav-text">从 Hello,World WordCount开始</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#flink执行环境"><span class="nav-number">2.1.</span> <span class="nav-text">flink执行环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算子（Operator）的注册（声明）"><span class="nav-number">2.2.</span> <span class="nav-text">算子（Operator）的注册（声明）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#程序的执行"><span class="nav-number">2.3.</span> <span class="nav-text">程序的执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#本地模式下的execute方法"><span class="nav-number">2.3.1.</span> <span class="nav-text">本地模式下的execute方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#远程模式（RemoteEnvironment）的execute方法"><span class="nav-number">2.3.2.</span> <span class="nav-text">远程模式（RemoteEnvironment）的execute方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#程序启动过程"><span class="nav-number">2.3.3.</span> <span class="nav-text">程序启动过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#理解flink的图结构"><span class="nav-number">3.</span> <span class="nav-text">理解flink的图结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#flink的三层图结构"><span class="nav-number">3.1.</span> <span class="nav-text">flink的三层图结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StreamGraph的生成"><span class="nav-number">3.2.</span> <span class="nav-text">StreamGraph的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#StreamTransformation类代表了流的转换"><span class="nav-number">3.2.1.</span> <span class="nav-text">StreamTransformation类代表了流的转换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#StreamGraph生成函数分析"><span class="nav-number">3.2.2.</span> <span class="nav-text">StreamGraph生成函数分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#WordCount函数的StreamGraph"><span class="nav-number">3.2.3.</span> <span class="nav-text">WordCount函数的StreamGraph</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JobGraph的生成"><span class="nav-number">3.3.</span> <span class="nav-text">JobGraph的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JobGraph生成源码"><span class="nav-number">3.3.1.</span> <span class="nav-text">JobGraph生成源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#operator-chain的逻辑"><span class="nav-number">3.3.2.</span> <span class="nav-text">operator chain的逻辑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-JobGraph的提交"><span class="nav-number">3.3.3.</span> <span class="nav-text">2.3.3 JobGraph的提交</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ExecutionGraph的生成"><span class="nav-number">3.4.</span> <span class="nav-text">ExecutionGraph的生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#任务的调度与执行"><span class="nav-number">4.</span> <span class="nav-text">任务的调度与执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#计算资源的调度"><span class="nav-number">4.1.</span> <span class="nav-text">计算资源的调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JobManager执行job"><span class="nav-number">4.2.</span> <span class="nav-text">JobManager执行job</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JobManager的组件"><span class="nav-number">4.2.1.</span> <span class="nav-text">JobManager的组件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JobManager的启动过程"><span class="nav-number">4.2.2.</span> <span class="nav-text">JobManager的启动过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JobManager启动Task"><span class="nav-number">4.2.3.</span> <span class="nav-text">JobManager启动Task</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TaskManager执行task"><span class="nav-number">4.3.</span> <span class="nav-text">TaskManager执行task</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TaskManager的基本组件"><span class="nav-number">4.3.1.</span> <span class="nav-text">TaskManager的基本组件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TaskManager执行Task"><span class="nav-number">4.3.2.</span> <span class="nav-text">TaskManager执行Task</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-1-生成Task对象"><span class="nav-number">4.3.3.</span> <span class="nav-text">3.3.2.1 生成Task对象</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#运行Task对象"><span class="nav-number">4.3.4.</span> <span class="nav-text">运行Task对象</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#StreamTask的执行逻辑"><span class="nav-number">4.3.5.</span> <span class="nav-text">StreamTask的执行逻辑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StreamTask与StreamOperator"><span class="nav-number">4.4.</span> <span class="nav-text">StreamTask与StreamOperator</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#StreamOperator的抽象与实现"><span class="nav-number">5.</span> <span class="nav-text">StreamOperator的抽象与实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据源的逻辑——StreamSource与时间模型"><span class="nav-number">5.1.</span> <span class="nav-text">数据源的逻辑——StreamSource与时间模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从数据输入到数据处理——OneInputStreamOperator-amp-AbstractUdfStreamOperator"><span class="nav-number">5.2.</span> <span class="nav-text">从数据输入到数据处理——OneInputStreamOperator &amp; AbstractUdfStreamOperator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StreamSink"><span class="nav-number">5.3.</span> <span class="nav-text">StreamSink</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为执行保驾护航——Fault-Tolerant与保证Exactly-Once语义"><span class="nav-number">6.</span> <span class="nav-text">为执行保驾护航——Fault Tolerant与保证Exactly-Once语义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fault-Tolerant演进之路"><span class="nav-number">6.1.</span> <span class="nav-text">Fault Tolerant演进之路</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Storm的Record-acknowledgement模式"><span class="nav-number">6.1.1.</span> <span class="nav-text">Storm的Record acknowledgement模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-streaming的micro-batch模式"><span class="nav-number">6.1.2.</span> <span class="nav-text">Spark streaming的micro batch模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Google-Cloud-Dataflow的事务式模型"><span class="nav-number">6.1.3.</span> <span class="nav-text">Google Cloud Dataflow的事务式模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink的分布式快照机制"><span class="nav-number">6.1.4.</span> <span class="nav-text">Flink的分布式快照机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#checkpoint的生命周期"><span class="nav-number">6.2.</span> <span class="nav-text">checkpoint的生命周期</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#触发checkpoint"><span class="nav-number">6.2.1.</span> <span class="nav-text">触发checkpoint</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Task层面checkpoint的准备工作"><span class="nav-number">6.2.2.</span> <span class="nav-text">Task层面checkpoint的准备工作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#操作符的状态保存及barrier传递"><span class="nav-number">6.2.3.</span> <span class="nav-text">操作符的状态保存及barrier传递</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#承载checkpoint数据的抽象：State-amp-StateBackend"><span class="nav-number">6.3.</span> <span class="nav-text">承载checkpoint数据的抽象：State &amp; StateBackend</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据流转——Flink的数据抽象及数据交换过程"><span class="nav-number">7.</span> <span class="nav-text">数据流转——Flink的数据抽象及数据交换过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#flink的数据抽象"><span class="nav-number">7.1.</span> <span class="nav-text">flink的数据抽象</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MemorySegment"><span class="nav-number">7.1.1.</span> <span class="nav-text">MemorySegment</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ByteBuffer与NetworkBufferPool"><span class="nav-number">7.1.2.</span> <span class="nav-text">ByteBuffer与NetworkBufferPool</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RecordWriter与Record"><span class="nav-number">7.1.3.</span> <span class="nav-text">RecordWriter与Record</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据流转过程"><span class="nav-number">7.2.</span> <span class="nav-text">数据流转过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#整体过程"><span class="nav-number">7.2.1.</span> <span class="nav-text">整体过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据跨task传递"><span class="nav-number">7.2.2.</span> <span class="nav-text">数据跨task传递</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Credit漫谈"><span class="nav-number">7.3.</span> <span class="nav-text">Credit漫谈</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背压问题"><span class="nav-number">7.3.1.</span> <span class="nav-text">背压问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用Credit实现ATM网络流控"><span class="nav-number">7.3.2.</span> <span class="nav-text">使用Credit实现ATM网络流控</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他核心概念"><span class="nav-number">8.</span> <span class="nav-text">其他核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#EventTime时间模型"><span class="nav-number">8.1.</span> <span class="nav-text">EventTime时间模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FLIP-6-部署及处理模型演进"><span class="nav-number">8.2.</span> <span class="nav-text">FLIP-6 部署及处理模型演进</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#现有模型不足"><span class="nav-number">8.2.1.</span> <span class="nav-text">现有模型不足</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核心变更"><span class="nav-number">8.2.2.</span> <span class="nav-text">核心变更</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cluster-Manager的架构"><span class="nav-number">8.2.3.</span> <span class="nav-text">Cluster Manager的架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#组件设计及细节"><span class="nav-number">8.2.4.</span> <span class="nav-text">组件设计及细节</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后记"><span class="nav-number">9.</span> <span class="nav-text">后记</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">2017-2018 Xavier Ma</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div>




        




  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65996570";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://maxavier.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://blog.daas.ai/2018/09/18/flink-code-section-1/';
          this.page.identifier = '2018/09/18/flink-code-section-1/';
          this.page.title = '追源索骥：透过源码看懂Flink核心框架的执行流程';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://maxavier.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
